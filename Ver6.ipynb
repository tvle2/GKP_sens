{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Setting configuration parameters.\n",
      "Configuration set. Scale = 1.7724538509055159\n",
      "Step 3: Preparing grid parameters for dataset generation.\n"
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from scipy.stats import skew, kurtosis\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm   # <-- Added import for progress bar\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION: ALL PARAMETERS USED IN THE PIPELINE\n",
    "# ==============================================================================\n",
    "PIPELINE_CONFIG = {\n",
    "    \"sf_hbar\": 1,\n",
    "    \"quad_range\": (-6, 6),\n",
    "    \"quad_points\": 200,  # for state simulation Wigner, not directly for measurements\n",
    "    \"num_bins\": 100,     # not used anymore for feature extraction\n",
    "    \"num_shots\": 100,    # homodyne measurements per grid point (not used for these marginal features)\n",
    "    \"grid\": {\n",
    "        \"num_db\": 20,       # distinct squeezing values (in dB)\n",
    "        \"num_loss\": 20,     # distinct loss values\n",
    "        \"num_gamma\": 20     # distinct dephasing values\n",
    "    },\n",
    "    \"db_range\": (10.0, 13.0),   # dB values range\n",
    "    \"loss_range\": (0.8, 1.0),     # loss values range\n",
    "    \"gamma_range\": (0.8, 1.0),    # dephasing values range\n",
    "    \"model\": {\n",
    "        \"input_shape\": (10,),        \n",
    "        \"loss_bins\": 20,\n",
    "        \"dephasing_bins\": 20,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 1000,\n",
    "        \"batch_size\": 32,\n",
    "        \"validation_split\": 0.2\n",
    "    },\n",
    "    \"save_paths\": {\n",
    "        \"dataset\": \"dataset.npz\",\n",
    "        \"model\": \"trained_model.h5\",\n",
    "        \"config\": \"pipeline_config.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the pipeline configuration to disk.\n",
    "with open(PIPELINE_CONFIG[\"save_paths\"][\"config\"], \"w\") as f:\n",
    "    json.dump(PIPELINE_CONFIG, f, indent=4)\n",
    "\n",
    "# ==============================================================================\n",
    "# SETUP: CONFIGURATION PARAMETERS AND GLOBAL VARIABLES\n",
    "# ==============================================================================\n",
    "print(\"Step 1: Setting configuration parameters.\")\n",
    "CONFIG = {\n",
    "    \"hbar\": PIPELINE_CONFIG[\"sf_hbar\"],\n",
    "    \"quad_range\": PIPELINE_CONFIG[\"quad_range\"],\n",
    "    \"quad_points\": PIPELINE_CONFIG[\"quad_points\"],\n",
    "    \"cmap\": plt.cm.RdBu,\n",
    "    \"colors\": {\n",
    "        \"q_marginal\": \"navy\",\n",
    "        \"p_marginal\": \"maroon\",\n",
    "        \"samples_x\": \"skyblue\",\n",
    "        \"samples_p\": \"lightcoral\",\n",
    "        \"ideal\": \"red\",\n",
    "        \"noisy\": \"green\"\n",
    "    }\n",
    "}\n",
    "sf.hbar = CONFIG[\"hbar\"]\n",
    "CONFIG[\"scale\"] = np.sqrt(CONFIG[\"hbar\"] * np.pi)\n",
    "print(\"Configuration set. Scale =\", CONFIG[\"scale\"])\n",
    "\n",
    "# ==============================================================================\n",
    "# UTILITY: dB to epsilon conversion and feature extraction\n",
    "# ==============================================================================\n",
    "def db_to_epsilon(db_val: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert a given GKP squeezing level in dB to epsilon via:\n",
    "         tanh(epsilon) = 10^(-db_val/10)\n",
    "    \"\"\"\n",
    "    t = 10.0 ** (-db_val / 10.0)\n",
    "    eps = 0.5 * np.log((1.0 + t) / (1.0 - t))\n",
    "    return eps\n",
    "\n",
    "def extract_statistical_features(samples_x: np.ndarray, samples_p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extract mean, variance, skewness, and kurtosis for X and P quadratures.\"\"\"\n",
    "    x_features = [\n",
    "        np.mean(samples_x), np.var(samples_x),\n",
    "        skew(samples_x), kurtosis(samples_x)\n",
    "    ]\n",
    "    p_features = [\n",
    "        np.mean(samples_p), np.var(samples_p),\n",
    "        skew(samples_p), kurtosis(samples_p)\n",
    "    ]\n",
    "    return np.array(x_features + p_features)\n",
    "\n",
    "def extract_marginal_features(prep_state: List[float], epsilon: float, noise_params: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract features from the marginal distributions:\n",
    "      - Compute ideal and noisy marginals (for both X and P)\n",
    "      - Compute statistical moments (mean, var, skew, kurtosis) from the noisy marginals\n",
    "      - Compute a difference metric (mean absolute difference) between ideal and noisy marginals\n",
    "    Returns a feature vector of length 10.\n",
    "    \"\"\"\n",
    "    # Compute marginals using provided functions.\n",
    "    ideal_x, ideal_p = calculate_ideal_marginals(prep_state, epsilon)\n",
    "    noisy_x, noisy_p = calculate_noisy_marginals(prep_state, epsilon, noise_params)\n",
    "    \n",
    "    # Compute moments for the noisy marginals.\n",
    "    features = [\n",
    "        np.mean(noisy_x), np.var(noisy_x), skew(noisy_x), kurtosis(noisy_x),\n",
    "        np.mean(noisy_p), np.var(noisy_p), skew(noisy_p), kurtosis(noisy_p)\n",
    "    ]\n",
    "    # Compute difference metrics (mean absolute difference).\n",
    "    diff_x = np.mean(np.abs(ideal_x - noisy_x))\n",
    "    diff_p = np.mean(np.abs(ideal_p - noisy_p))\n",
    "    features.extend([diff_x, diff_p])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# ==============================================================================\n",
    "# CORE SIMULATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "def simulate_gkp(\n",
    "    prep_state: List[float],\n",
    "    epsilons: List[float],\n",
    "    noise_params: Dict = None,\n",
    "    num_samples: int = 1\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Simulate GKP states with optional noise channels.\n",
    "    Returns lists of Wigner functions and marginals for each epsilon.\n",
    "    \"\"\"\n",
    "    noise_params = noise_params or {}\n",
    "    quad = np.linspace(*CONFIG[\"quad_range\"], CONFIG[\"quad_points\"]) * CONFIG[\"scale\"]\n",
    "    \n",
    "    wigners, marginals_q, marginals_p = [], [], []\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        print(f\"Processing Îµ={epsilon}...\")\n",
    "        avg_wigner = np.zeros((len(quad), len(quad)))\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            prog = sf.Program(1)\n",
    "            with prog.context as q:\n",
    "                sf.ops.GKP(state=prep_state, epsilon=epsilon) | q\n",
    "                if 'loss' in noise_params:\n",
    "                    sf.ops.LossChannel(noise_params['loss']) | q\n",
    "                if 'gamma' in noise_params:\n",
    "                    theta = np.random.normal(0, np.sqrt(2 * noise_params['gamma']))\n",
    "                    sf.ops.Rgate(theta) | q\n",
    "            eng = sf.Engine(\"bosonic\")\n",
    "            state = eng.run(prog).state\n",
    "            if num_samples > 1:\n",
    "                avg_wigner += state.wigner(0, quad, quad)\n",
    "            else:\n",
    "                avg_wigner = state.wigner(0, quad, quad)\n",
    "        \n",
    "        wigners.append(avg_wigner / num_samples if num_samples > 1 else avg_wigner)\n",
    "        marginals_q.append(state.marginal(0, quad, phi=0))\n",
    "        marginals_p.append(state.marginal(0, quad, phi=np.pi/2))\n",
    "        \n",
    "    return wigners, marginals_q, marginals_p\n",
    "\n",
    "def simulate_homodyne(prep_state: List[float], epsilon: float, noise_params: Dict, num_samples: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate homodyne measurements (X and P quadratures) for a given epsilon and noise parameters.\n",
    "    Returns two arrays of measurement outcomes.\n",
    "    \"\"\"\n",
    "    samples_x, samples_p = [], []\n",
    "    for _ in range(num_samples):\n",
    "        prog_x = sf.Program(1)\n",
    "        with prog_x.context as q:\n",
    "            sf.ops.GKP(state=prep_state, epsilon=epsilon) | q\n",
    "            if 'loss' in noise_params:\n",
    "                sf.ops.LossChannel(noise_params['loss']) | q\n",
    "            if 'gamma' in noise_params:\n",
    "                theta = np.random.normal(0, np.sqrt(2 * noise_params['gamma']))\n",
    "                sf.ops.Rgate(theta) | q\n",
    "            sf.ops.MeasureX | q\n",
    "        eng_x = sf.Engine(\"bosonic\")\n",
    "        sample_x = eng_x.run(prog_x).samples[0, 0] / CONFIG[\"scale\"]\n",
    "        samples_x.append(sample_x)\n",
    "        \n",
    "        prog_p = sf.Program(1)\n",
    "        with prog_p.context as q:\n",
    "            sf.ops.GKP(state=prep_state, epsilon=epsilon) | q\n",
    "            if 'loss' in noise_params:\n",
    "                sf.ops.LossChannel(noise_params['loss']) | q\n",
    "            if 'gamma' in noise_params:\n",
    "                theta = np.random.normal(0, np.sqrt(2 * noise_params['gamma']))\n",
    "                sf.ops.Rgate(theta) | q\n",
    "            sf.ops.MeasureP | q\n",
    "        eng_p = sf.Engine(\"bosonic\")\n",
    "        sample_p = eng_p.run(prog_p).samples[0, 0] / CONFIG[\"scale\"]\n",
    "        samples_p.append(sample_p)\n",
    "    \n",
    "    return np.array(samples_x), np.array(samples_p)\n",
    "\n",
    "def calculate_ideal_marginals(prep_state: List[float], epsilon: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calculate ideal marginal distributions.\"\"\"\n",
    "    prog = sf.Program(1)\n",
    "    with prog.context as q:\n",
    "        sf.ops.GKP(state=prep_state, epsilon=epsilon) | q\n",
    "    state = sf.Engine(\"bosonic\").run(prog).state\n",
    "    quad_axis = np.linspace(-6, 6, 1000) * CONFIG[\"scale\"]\n",
    "    return (\n",
    "        state.marginal(0, quad_axis, phi=0) * CONFIG[\"scale\"],\n",
    "        state.marginal(0, quad_axis, phi=np.pi/2) * CONFIG[\"scale\"]\n",
    "    )\n",
    "\n",
    "def calculate_noisy_marginals(prep_state: List[float], epsilon: float, noise_params: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calculate averaged noisy marginals.\"\"\"\n",
    "    quad_axis = np.linspace(-6, 6, 1000) * CONFIG[\"scale\"]\n",
    "    num_avg = 100\n",
    "    marg_x, marg_p = np.zeros_like(quad_axis), np.zeros_like(quad_axis)\n",
    "    \n",
    "    for _ in range(num_avg):\n",
    "        prog = sf.Program(1)\n",
    "        with prog.context as q:\n",
    "            sf.ops.GKP(state=prep_state, epsilon=epsilon) | q\n",
    "            if 'loss' in noise_params:\n",
    "                sf.ops.LossChannel(noise_params['loss']) | q\n",
    "            if 'gamma' in noise_params:\n",
    "                theta = np.random.normal(0, np.sqrt(2 * noise_params['gamma']))\n",
    "                sf.ops.Rgate(theta) | q\n",
    "        state = sf.Engine(\"bosonic\").run(prog).state\n",
    "        marg_x += state.marginal(0, quad_axis, phi=0)\n",
    "        marg_p += state.marginal(0, quad_axis, phi=np.pi/2)\n",
    "        \n",
    "    return (marg_x/num_avg * CONFIG[\"scale\"], marg_p/num_avg * CONFIG[\"scale\"])\n",
    "\n",
    "def plot_quadratures(\n",
    "    samples_x: np.ndarray,\n",
    "    samples_p: np.ndarray,\n",
    "    ideal_x: np.ndarray,\n",
    "    ideal_p: np.ndarray,\n",
    "    noisy_x: np.ndarray,\n",
    "    noisy_p: np.ndarray,\n",
    "    title: str = \"\",\n",
    "    figsize: Tuple[float, float] = (12, 5)\n",
    "):\n",
    "    \"\"\"Plot quadrature measurement results\"\"\"\n",
    "    quad_axis = np.linspace(-6, 6, 1000)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # X quadrature\n",
    "    axs[0].hist(samples_x, bins=100, density=True, \n",
    "               color=CONFIG[\"colors\"][\"samples_x\"], label=\"Samples\")\n",
    "    # axs[0].plot(quad_axis, ideal_x, '--', color=CONFIG[\"colors\"][\"ideal\"], label=\"Ideal\")\n",
    "    axs[0].plot(quad_axis, noisy_x, '-', color=CONFIG[\"colors\"][\"noisy\"], label=\"Noisy\")\n",
    "    axs[0].set_xlabel(r\"$q$ ($\\sqrt{\\pi\\hbar}$)\", fontsize=12)\n",
    "    axs[0].set_ylabel(\"Probability Density\", fontsize=12)\n",
    "\n",
    "    # Add bin overlays for X quadrature\n",
    "    # Adjust j range to cover the plotted quad_axis (here, -3 to 3 covers roughly -6 to 6)\n",
    "    for j in range(-3, 4):\n",
    "        axs[0].axvspan(2*j - 0.5, 2*j + 0.5, alpha=0.2, facecolor='b')\n",
    "        axs[0].axvspan(2*j + 0.5, 2*j + 1.5, alpha=0.2, facecolor='r')\n",
    "    \n",
    "    # P quadrature\n",
    "    axs[1].hist(samples_p, bins=100, density=True, \n",
    "               color=CONFIG[\"colors\"][\"samples_p\"], label=\"Samples\")\n",
    "    # axs[1].plot(quad_axis, ideal_p, '--', color=CONFIG[\"colors\"][\"ideal\"], label=\"Ideal\")\n",
    "    axs[1].plot(quad_axis, noisy_p, '-', color=CONFIG[\"colors\"][\"noisy\"], label=\"Noisy\")\n",
    "    axs[1].set_xlabel(r\"$p$ ($\\sqrt{\\pi\\hbar}$)\", fontsize=12)\n",
    "\n",
    "    # Add bin overlays for P quadrature\n",
    "    for j in range(-3, 4):\n",
    "        axs[1].axvspan(2*j - 0.5, 2*j + 0.5, alpha=0.2, facecolor='b')\n",
    "        axs[1].axvspan(2*j + 0.5, 2*j + 1.5, alpha=0.2, facecolor='r')\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.legend()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# DEFINE THE FEED-FORWARD NEURAL NETWORK (FNN) MODEL\n",
    "# ==============================================================================\n",
    "def build_fnn_model_ver1(input_shape: Tuple[int], loss_bins: int, dephasing_bins: int) -> Model:\n",
    "    \"\"\"Build a feed-forward neural network for loss and dephasing prediction.\"\"\"\n",
    "    inputs = Input(shape=input_shape, name='statistical_features')\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    loss_output = Dense(loss_bins, activation='softmax', name='loss_output')(x)\n",
    "    dephasing_output = Dense(dephasing_bins, activation='softmax', name='dephasing_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[loss_output, dephasing_output])\n",
    "    return model\n",
    "\n",
    "def build_fnn_model(input_shape: Tuple[int], loss_bins: int, dephasing_bins: int) -> Model:\n",
    "    \"\"\"\n",
    "    Build a deeper feed-forward neural network for loss and dephasing prediction.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name='statistical_features')\n",
    "    \n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    loss_output = Dense(loss_bins, activation='softmax', name='loss_output')(x)\n",
    "    dephasing_output = Dense(dephasing_bins, activation='softmax', name='dephasing_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[loss_output, dephasing_output])\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# CREATE A GRID OF DISTINCT VALUES AND GENERATE THE DATASET\n",
    "# ==============================================================================\n",
    "print(\"Step 3: Preparing grid parameters for dataset generation.\")\n",
    "\n",
    "grid_config = PIPELINE_CONFIG[\"grid\"]\n",
    "num_db = grid_config[\"num_db\"]\n",
    "num_loss = grid_config[\"num_loss\"]\n",
    "num_gamma = grid_config[\"num_gamma\"]\n",
    "\n",
    "# Define the parameter ranges.\n",
    "db_values = np.linspace(PIPELINE_CONFIG[\"db_range\"][0], PIPELINE_CONFIG[\"db_range\"][1], num_db)\n",
    "loss_values = np.linspace(PIPELINE_CONFIG[\"loss_range\"][0], PIPELINE_CONFIG[\"loss_range\"][1], num_loss)\n",
    "gamma_values = np.linspace(PIPELINE_CONFIG[\"gamma_range\"][0], PIPELINE_CONFIG[\"gamma_range\"][1], num_gamma)\n",
    "\n",
    "# Define the two possible GKP preparation states.\n",
    "prep_states = [[0, 0], [np.pi, 0]]\n",
    "\n",
    "# Create a grid over all parameters: (prep_state, db, loss, gamma)\n",
    "grid_params = list(product(prep_states, db_values, loss_values, gamma_values))\n",
    "\n",
    "# For labeling, we use grid indices.\n",
    "loss_bins = num_loss\n",
    "dephasing_bins = num_gamma\n",
    "\n",
    "def process_grid_point(params):\n",
    "    prep_state, db_val, loss_val, gamma_val = params\n",
    "    epsilon = db_to_epsilon(db_val)\n",
    "    current_noise_params = {\"loss\": loss_val, \"gamma\": gamma_val}\n",
    "    # Instead of simulating theirhomodyne samples and extracting  moments,\n",
    "    # we extract features from the marginal distributions.\n",
    "    features = extract_marginal_features(prep_state, epsilon, current_noise_params)\n",
    "    # Determine grid indices for labels.\n",
    "    loss_idx = np.where(np.isclose(loss_values, loss_val))[0][0]\n",
    "    dephasing_idx = np.where(np.isclose(gamma_values, gamma_val))[0][0]\n",
    "    return features, loss_idx, dephasing_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PIPELINE_CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m \u001b[43mPIPELINE_CONFIG\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating examples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PIPELINE_CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_path = PIPELINE_CONFIG[\"save_paths\"][\"dataset\"]\n",
    "    print(\"Generating examples\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"No dataset found. Generating new dataset...\")\n",
    "        results = []\n",
    "        for params in tqdm(grid_params, total=len(grid_params)):\n",
    "            results.append(process_grid_point(params))\n",
    "        \n",
    "        X_data, y_loss, y_dephasing = zip(*results)\n",
    "        X_data = np.array(X_data)  # shape: (num_examples, 10)\n",
    "        y_loss = np.array(y_loss)\n",
    "        y_dephasing = np.array(y_dephasing)\n",
    "        num_examples = X_data.shape[0]\n",
    "        print(\"Total examples generated:\", num_examples)\n",
    "        \n",
    "        # Save the dataset.\n",
    "        np.savez_compressed(PIPELINE_CONFIG[\"save_paths\"][\"dataset\"],\n",
    "                            X_data=X_data, y_loss=y_loss, y_dephasing=y_dephasing)\n",
    "        print(\"Dataset saved to\", PIPELINE_CONFIG[\"save_paths\"][\"dataset\"])\n",
    "    \n",
    "    else:\n",
    "        print(f\"Dataset file found. Skipping dataset generation.\")\n",
    "        \n",
    "        # Load dataset directly\n",
    "        data = np.load(dataset_path)\n",
    "        X_data = data[\"X_data\"]\n",
    "        print(f\"Original dataset shape {X_data.shape}\")\n",
    "        X_data = X_data[:, :8]\n",
    "        print(f\"Droped marginal dataset shape {X_data.shape}\")\n",
    "        y_loss = data[\"y_loss\"]\n",
    "        y_dephasing = data[\"y_dephasing\"]\n",
    "        num_examples = X_data.shape[0]\n",
    "        print(\"Dataset loaded. Total examples:\", num_examples)\n",
    "        \n",
    "    # ==============================================================================\n",
    "    # SPLIT THE DATASET INTO TRAINING AND TEST SETS (80/20 split)\n",
    "    # ==============================================================================\n",
    "    print(\"Step 4: Splitting dataset into training and test sets.\")\n",
    "    split = int(0.8 * num_examples)\n",
    "    X_train, X_test = X_data[:split], X_data[split:]\n",
    "    y_loss_train, y_loss_test = y_loss[:split], y_loss[split:]\n",
    "    y_dephasing_train, y_dephasing_test = y_dephasing[:split], y_dephasing[split:]\n",
    "    print(\"Training examples:\", X_train.shape[0])\n",
    "    print(\"Test examples:\", X_test.shape[0])\n",
    "\n",
    "    # ==============================================================================\n",
    "    # BUILD AND COMPILE THE FNN MODEL\n",
    "    # ==============================================================================\n",
    "    print(\"Step 5: Building and compiling the FNN model.\")\n",
    "    model_config = PIPELINE_CONFIG[\"model\"]\n",
    "    input_shape = model_config[\"input_shape\"]\n",
    "    model = build_fnn_model(input_shape, model_config[\"loss_bins\"], model_config[\"dephasing_bins\"])\n",
    "    model.compile(optimizer=Adam(learning_rate=model_config[\"learning_rate\"]),\n",
    "                  loss={'loss_output': 'sparse_categorical_crossentropy',\n",
    "                        'dephasing_output': 'sparse_categorical_crossentropy'},\n",
    "                  metrics={'loss_output': 'accuracy',\n",
    "                           'dephasing_output': 'accuracy'})\n",
    "    model.summary()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # TRAIN THE MODEL\n",
    "    # ==============================================================================\n",
    "    print(\"Step 6: Training the model.\")\n",
    "    history = model.fit(X_train, \n",
    "                        {'loss_output': y_loss_train, 'dephasing_output': y_dephasing_train},\n",
    "                        epochs=model_config[\"epochs\"], \n",
    "                        batch_size=model_config[\"batch_size\"], \n",
    "                        validation_split=model_config[\"validation_split\"], \n",
    "                        verbose=1)\n",
    "\n",
    "    # Save the trained model.\n",
    "    model.save(PIPELINE_CONFIG[\"save_paths\"][\"model\"])\n",
    "    print(\"Trained model saved to\", PIPELINE_CONFIG[\"save_paths\"][\"model\"])\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PLOT TRAINING ACCURACY AND LOSS CURVES\n",
    "    # ==============================================================================\n",
    "    print(\"Step 7: Plotting training accuracy and loss curves.\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history.history['loss_output_accuracy'], label='Train Loss Accuracy')\n",
    "    plt.plot(history.history['val_loss_output_accuracy'], label='Val Loss Accuracy')\n",
    "    plt.plot(history.history['dephasing_output_accuracy'], label='Train Dephasing Accuracy')\n",
    "    plt.plot(history.history['val_dephasing_output_accuracy'], label='Val Dephasing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy During Training')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Total Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Total Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # EVALUATE THE MODEL ON THE TEST SET\n",
    "    # ==============================================================================\n",
    "    print(\"Step 8: Evaluating the model on the test set.\")\n",
    "    def evaluate_model(model, X_test, y_loss_test, y_dephasing_test):\n",
    "        pred_loss_probs, pred_dephasing_probs = model.predict(X_test)\n",
    "        pred_loss = np.argmax(pred_loss_probs, axis=1)\n",
    "        pred_dephasing = np.argmax(pred_dephasing_probs, axis=1)\n",
    "        \n",
    "        loss_accuracy = accuracy_score(y_loss_test, pred_loss)\n",
    "        dephasing_accuracy = accuracy_score(y_dephasing_test, pred_dephasing)\n",
    "        \n",
    "        print(\"Loss Prediction Accuracy: {:.2f}%\".format(loss_accuracy * 100))\n",
    "        print(\"Dephasing Prediction Accuracy: {:.2f}%\".format(dephasing_accuracy * 100))\n",
    "        \n",
    "        print(\"\\nLoss Classification Report:\")\n",
    "        print(classification_report(y_loss_test, pred_loss))\n",
    "        print(\"\\nDephasing Classification Report:\")\n",
    "        print(classification_report(y_dephasing_test, pred_dephasing))\n",
    "        \n",
    "        # Plot confusion matrices\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        cm_loss = confusion_matrix(y_loss_test, pred_loss)\n",
    "        sns.heatmap(cm_loss, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
    "        axes[0].set_title(\"Loss Parameter Confusion Matrix\")\n",
    "        axes[0].set_xlabel(\"Predicted Bin\")\n",
    "        axes[0].set_ylabel(\"True Bin\")\n",
    "        \n",
    "        cm_dephasing = confusion_matrix(y_dephasing_test, pred_dephasing)\n",
    "        sns.heatmap(cm_dephasing, annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
    "        axes[1].set_title(\"Dephasing Parameter Confusion Matrix\")\n",
    "        axes[1].set_xlabel(\"Predicted Bin\")\n",
    "        axes[1].set_ylabel(\"True Bin\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    evaluate_model(model, X_test, y_loss_test, y_dephasing_test)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # BAYESIAN UPDATE FUNCTION FOR COMBINING MULTIPLE INDEPENDENT MEASUREMENTS\n",
    "    # ==============================================================================\n",
    "    print(\"Step 9: Defining Bayesian update function.\")\n",
    "    def bayesian_update(model, X_samples, y_loss_true=None, y_dephasing_true=None, \n",
    "                        loss_bin_centers=None, dephasing_bin_centers=None):\n",
    "        \"\"\"\n",
    "        Given a list of independent FNN inputs (X_samples) corresponding to repeated measurements\n",
    "        for the same channel setting, compute the combined posterior probability for each output.\n",
    "        Optionally print estimated and true parameter values if bin centers and true labels are provided.\n",
    "        \"\"\"\n",
    "        pred_loss_probs, pred_dephasing_probs = model.predict(X_samples[0][np.newaxis, ...])\n",
    "        combined_loss_post = pred_loss_probs.flatten()\n",
    "        combined_dephasing_post = pred_dephasing_probs.flatten()\n",
    "        \n",
    "        for X in X_samples[1:]:\n",
    "            pred_loss, pred_dephasing = model.predict(X[np.newaxis, ...])\n",
    "            combined_loss_post *= pred_loss.flatten()\n",
    "            combined_dephasing_post *= pred_dephasing.flatten()\n",
    "        \n",
    "        combined_loss_post /= np.sum(combined_loss_post)\n",
    "        combined_dephasing_post /= np.sum(combined_dephasing_post)\n",
    "        \n",
    "        est_loss_bin = np.argmax(combined_loss_post)\n",
    "        est_dephasing_bin = np.argmax(combined_dephasing_post)\n",
    "        \n",
    "        print(\"Bayesian Update: MAP estimate for Loss Bin:\", est_loss_bin)\n",
    "        print(\"Bayesian Update: MAP estimate for Dephasing Bin:\", est_dephasing_bin)\n",
    "        \n",
    "        if loss_bin_centers is not None and y_loss_true is not None:\n",
    "            est_loss_value = loss_bin_centers[est_loss_bin]\n",
    "            true_loss_value = loss_bin_centers[y_loss_true]\n",
    "            print(\"Estimated Loss Value: {:.4f} (True: {:.4f})\".format(est_loss_value, true_loss_value))\n",
    "        if dephasing_bin_centers is not None and y_dephasing_true is not None:\n",
    "            est_dephasing_value = dephasing_bin_centers[est_dephasing_bin]\n",
    "            true_dephasing_value = dephasing_bin_centers[y_dephasing_true]\n",
    "            print(\"Estimated Dephasing Value: {:.4f} (True: {:.4f})\".format(est_dephasing_value, true_dephasing_value))\n",
    "        \n",
    "        return combined_loss_post, combined_dephasing_post\n",
    "\n",
    "    # ==============================================================================\n",
    "    # EXAMPLE USAGE OF THE BAYESIAN UPDATE AFTER TRAINING\n",
    "    # ==============================================================================\n",
    "    print(\"Step 10: Demonstrating Bayesian update on repeated measurements.\")\n",
    "    X_test_list = []\n",
    "    num_bayes_samples = 5  # number of independent measurements\n",
    "    \n",
    "    # For demonstration, use the parameters of the first test example.\n",
    "    db_val_example = db_values[0]\n",
    "    loss_val_example = loss_values[y_loss_test[0]]\n",
    "    gamma_val_example = gamma_values[y_dephasing_test[0]]\n",
    "    epsilon_example = db_to_epsilon(db_val_example)\n",
    "    current_noise_params_example = {\"loss\": loss_val_example, \"gamma\": gamma_val_example}\n",
    "    \n",
    "    demo_prep_state = [0, 0]\n",
    "    \n",
    "    for _ in range(num_bayes_samples):\n",
    "        # We use the marginal-feature extraction here too.\n",
    "        features = extract_marginal_features(demo_prep_state, epsilon_example, current_noise_params_example)\n",
    "        X_test_list.append(features)\n",
    "    combined_loss_post, combined_dephasing_post = bayesian_update(\n",
    "        model, X_test_list, \n",
    "        y_loss_true=y_loss_test[0], y_dephasing_true=y_dephasing_test[0],\n",
    "        loss_bin_centers=loss_values, dephasing_bin_centers=gamma_values\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_Computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
