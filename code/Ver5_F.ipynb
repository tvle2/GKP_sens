{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import strawberryfields as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GKP_SPACING = np.sqrt(np.pi)\n",
    "\n",
    "def preprocess_homodyne_data(measurements):\n",
    "    mean = np.mean(measurements)\n",
    "    std = np.std(measurements)\n",
    "    normalized = (measurements - mean) / std\n",
    "\n",
    "    modular_residual = normalized % GKP_SPACING\n",
    "    sin_feature = np.sin(2 * np.pi * modular_residual / GKP_SPACING)\n",
    "    cos_feature = np.cos(2 * np.pi * modular_residual / GKP_SPACING)\n",
    "\n",
    "    features = np.vstack((normalized, sin_feature, cos_feature)).T\n",
    "    return features\n",
    "\n",
    "# Convert GKP dB -> epsilon\n",
    "def db_to_epsilon(db_val: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert a given GKP squeezing level in dB to the corresponding epsilon,\n",
    "    defined via tanh(epsilon) = 10^(-db_val/10).\n",
    "\n",
    "    For example:\n",
    "      db_val=10  => tanh(eps)=0.1   => eps ~ 0.100167\n",
    "      db_val=15  => tanh(eps)=0.0316 => eps ~ 0.032\n",
    "    \"\"\"\n",
    "    t = 10.0 ** (-db_val / 10.0)  # tanh(epsilon) = t\n",
    "    # epsilon = arctanh(t) = 0.5 * ln((1+t)/(1-t))\n",
    "    eps = 0.5 * np.log((1.0 + t)/(1.0 - t))\n",
    "    return eps\n",
    "\n",
    "# Prepare GKP + channel => Wigner tomography\n",
    "def compute_wigner_tomography(\n",
    "    epsilon: float,\n",
    "    in_state=(0.0, 0.0),\n",
    "    eta=1.0,\n",
    "    gamma=0.0,\n",
    "    q_pts=50,\n",
    "    range_q=(-5.0,5.0),\n",
    "    range_p=(-5.0,5.0),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare a single-mode GKP(epsilon, in_state), apply pure-loss channel(eta),\n",
    "    then approximate the dephasing channel by integrating over many rotation angles\n",
    "    phi ~ Normal(0, gamma). Returns a 2D Wigner array of shape (q_pts, q_pts).\n",
    "    \"\"\"\n",
    "    qvals = np.linspace(range_q[0], range_q[1], q_pts)\n",
    "    pvals = np.linspace(range_p[0], range_p[1], q_pts)\n",
    "\n",
    "    # If gamma is extremely small, skip the rotation sampling\n",
    "    if gamma < 1e-12:\n",
    "        prog = sf.Program(1)\n",
    "        with prog.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "        eng = sf.Engine(backend)\n",
    "        state = eng.run(prog).state\n",
    "        return state.wigner(0, qvals, pvals)\n",
    "\n",
    "    # Otherwise, discretize phi in ± phi_clip * sqrt(gamma)\n",
    "    phi_std = np.sqrt(gamma)\n",
    "    phi_min = -phi_clip * phi_std\n",
    "    phi_max = +phi_clip * phi_std\n",
    "    phis = np.linspace(phi_min, phi_max, Nphi)\n",
    "    dphi = phis[1] - phis[0]  # step size\n",
    "\n",
    "    # Gaussian weights\n",
    "    w_phi = np.exp(-0.5 * (phis/phi_std)**2)\n",
    "    w_phi /= (phi_std * np.sqrt(2*np.pi))  # normalization\n",
    "\n",
    "    W_accum = np.zeros((q_pts, q_pts), dtype=float)\n",
    "    for i, phi_val in enumerate(phis):\n",
    "        prog_phi = sf.Program(1)\n",
    "        with prog_phi.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "            sf.ops.Rgate(phi_val) | q[0]\n",
    "\n",
    "        eng_phi = sf.Engine(backend)\n",
    "        state_phi = eng_phi.run(prog_phi).state\n",
    "        W_phi = state_phi.wigner(0, qvals, pvals)\n",
    "\n",
    "        W_accum += w_phi[i]*W_phi\n",
    "\n",
    "    # Multiply by dphi to approximate integral\n",
    "    return W_accum * dphi\n",
    "\n",
    "# Dataset: random (eta, gamma, dB) => flattened Wigner + epsilon\n",
    "class GKPChannelDatasetWithEps(Dataset):\n",
    "    \"\"\"\n",
    "    Samples random (eta, gamma, dB) in given ranges,\n",
    "    simulates the Wigner function for GKP(epsilon) with that dB => epsilon.\n",
    "    The final input X is shape (q_pts^2 + 1), i.e. [W_flat, epsilon].\n",
    "    The label Y is shape (2), i.e. [eta, gamma].\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        eta_range=(0.5, 1.0),\n",
    "        gamma_range=(0.0, 1.0),\n",
    "        db_range=(10.0, 15.0),\n",
    "        q_pts=50,\n",
    "        range_q=(-5.0,5.0),\n",
    "        range_p=(-5.0,5.0),\n",
    "        in_state=(0.0, 0.0),\n",
    "        backend=\"bosonic\",\n",
    "        Nphi=5,\n",
    "        phi_clip=3.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "\n",
    "        # Randomly sample the channel + GKP parameters\n",
    "        etas    = np.random.uniform(eta_range[0],   eta_range[1],   N)\n",
    "        gammas  = np.random.uniform(gamma_range[0], gamma_range[1], N)\n",
    "        db_vals = np.random.uniform(db_range[0],    db_range[1],    N)\n",
    "\n",
    "        # Generate each training sample\n",
    "        for i in range(N):\n",
    "            eps_val = db_to_epsilon(db_vals[i])\n",
    "            W = compute_wigner_tomography(\n",
    "                epsilon=eps_val,\n",
    "                in_state=in_state,\n",
    "                eta=etas[i],\n",
    "                gamma=gammas[i],\n",
    "                q_pts=q_pts,\n",
    "                range_q=range_q,\n",
    "                range_p=range_p,\n",
    "                backend=backend,\n",
    "                Nphi=Nphi,\n",
    "                phi_clip=phi_clip\n",
    "            )\n",
    "\n",
    "            # Flatten Wigner => shape (q_pts^2,)\n",
    "            W_flat = W.flatten().astype(np.float32)\n",
    "            # We'll store X = [W_flat, eps_val]\n",
    "            x_vector = np.concatenate([W_flat, [eps_val]], axis=0).astype(np.float32)\n",
    "            # Y = [eta, gamma]\n",
    "            y_vector = np.array([etas[i], gammas[i]], dtype=np.float32)\n",
    "\n",
    "            x_list.append(x_vector)\n",
    "            y_list.append(y_vector)\n",
    "\n",
    "        self.x_data = np.array(x_list, dtype=np.float32)  # shape (N, q_pts^2 + 1)\n",
    "        self.y_data = np.array(y_list, dtype=np.float32)  # shape (N, 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "# 4) Neural Net: (q_pts^2 + 1) => [eta, gamma]\n",
    "class SimpleRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps [flattened Wigner, epsilon] => [eta, gamma].\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # final => [eta, gamma]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 5) Training: Create dataset, train model\n",
    "def train_model_with_eps(\n",
    "    N_train=5000,\n",
    "    N_test=500,\n",
    "    q_pts=32,\n",
    "    range_q=(-6,6),\n",
    "    range_p=(-6,6),\n",
    "    eta_range=(0.5, 1.0),\n",
    "    gamma_range=(0.0, 1.0),\n",
    "    db_range=(10.0, 15.0),\n",
    "    n_epochs=30,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    hidden_dim=128,\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset with random (eta, gamma, dB),\n",
    "    train the model, return (model, train_losses, test_losses).\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = GKPChannelDatasetWithEps(\n",
    "        N=N_train,\n",
    "        eta_range=eta_range,\n",
    "        gamma_range=gamma_range,\n",
    "        db_range=db_range,\n",
    "        q_pts=q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=\"bosonic\",\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "    test_dataset = GKPChannelDatasetWithEps(\n",
    "        N=N_test,\n",
    "        eta_range=eta_range,\n",
    "        gamma_range=gamma_range,\n",
    "        db_range=db_range,\n",
    "        q_pts=q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=\"bosonic\",\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Build model\n",
    "    input_dim = q_pts*q_pts + 1  # Flattened Wigner + 1 scalar epsilon\n",
    "    model = SimpleRegressor(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "    # Loss & Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train\n",
    "    train_losses = []\n",
    "    test_losses  = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                preds = model(x_batch)\n",
    "                loss_val = criterion(preds, y_batch)\n",
    "                test_loss_val += loss_val.item() * x_batch.size(0)\n",
    "        epoch_test_loss = test_loss_val / len(test_loader.dataset)\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "              f\"train MSE={epoch_train_loss:.6f}, test MSE={epoch_test_loss:.6f}\")\n",
    "\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "\n",
    "# 6) Plot training performance\n",
    "\n",
    "def plot_training_performance(train_losses, test_losses):\n",
    "    \"\"\"\n",
    "    Plots the training vs. test MSE across epochs.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, test_losses, label=\"Test Loss\", marker=\"s\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Model Training Performance\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 7) Single-example prediction using variable dB\n",
    "\n",
    "def predict_single_example_with_eps(\n",
    "    model: nn.Module,\n",
    "    eta_true: float,\n",
    "    gamma_true: float,\n",
    "    db_val: float,\n",
    "    q_pts=32,\n",
    "    range_q=(-6,6),\n",
    "    range_p=(-6,6),\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate one Wigner tomography for (eta_true, gamma_true, db_val => epsilon),\n",
    "    flatten + append epsilon, feed to model, return predicted (eta_pred, gamma_pred).\n",
    "    \"\"\"\n",
    "    eps_val = db_to_epsilon(db_val)\n",
    "    W_new = compute_wigner_tomography(\n",
    "        epsilon=eps_val,\n",
    "        in_state=(0.0, 0.0),\n",
    "        eta=eta_true,\n",
    "        gamma=gamma_true,\n",
    "        q_pts=q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=\"bosonic\",\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "\n",
    "    # Flatten & append epsilon\n",
    "    W_flat = W_new.flatten().astype(np.float32)\n",
    "    x_vec = np.concatenate([W_flat, [eps_val]], axis=0).astype(np.float32)\n",
    "    x_torch = torch.tensor(x_vec, dtype=torch.float32).unsqueeze(0)  # shape=(1, q_pts^2 + 1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x_torch).cpu().numpy()[0]\n",
    "    return out[0], out[1]  # (eta_pred, gamma_pred)\n",
    "\n",
    "\n",
    "# 8) Plot predictions for multiple (eta, gamma, dB)\n",
    "\n",
    "def plot_prediction_performance_with_eps(\n",
    "    model: nn.Module,\n",
    "    param_list,\n",
    "    q_pts=32,\n",
    "    range_q=(-6,6),\n",
    "    range_p=(-6,6),\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the model's predictions for multiple unseen parameters,\n",
    "    where (eta, gamma, dB) vary and epsilon is not fixed.\n",
    "\n",
    "    param_list: list of triples (eta_true, gamma_true, db_val).\n",
    "\n",
    "    We'll plot True vs. Pred for eta on the left subplot,\n",
    "    and True vs. Pred for gamma on the right subplot.\n",
    "    \"\"\"\n",
    "    etas_true   = []\n",
    "    gammas_true = []\n",
    "    etas_pred   = []\n",
    "    gammas_pred = []\n",
    "\n",
    "    for (eta_val, gamma_val, db_val) in param_list:\n",
    "        eta_p, gamma_p = predict_single_example_with_eps(\n",
    "            model,\n",
    "            eta_val, gamma_val,\n",
    "            db_val,\n",
    "            q_pts=q_pts,\n",
    "            range_q=range_q,\n",
    "            range_p=range_p,\n",
    "            Nphi=Nphi,\n",
    "            phi_clip=phi_clip\n",
    "        )\n",
    "        etas_true.append(eta_val)\n",
    "        gammas_true.append(gamma_val)\n",
    "        etas_pred.append(eta_p)\n",
    "        gammas_pred.append(gamma_p)\n",
    "\n",
    "    x_vals = range(len(param_list))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "    # Left subplot: ETA\n",
    "    axs[0].plot(x_vals, etas_true, 'o--', color=\"blue\", label=\"True Eta\")\n",
    "    axs[0].plot(x_vals, etas_pred, 's--', color=\"orange\", label=\"Pred Eta\")\n",
    "    axs[0].set_xlabel(\"Sample Index\")\n",
    "    axs[0].set_ylabel(\"Eta\")\n",
    "    axs[0].set_title(\"ETA: True vs. Predicted\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Right subplot: GAMMA\n",
    "    axs[1].plot(x_vals, gammas_true, 'o--', color=\"red\", label=\"True Gamma\")\n",
    "    axs[1].plot(x_vals, gammas_pred, 's--', color=\"green\", label=\"Pred Gamma\")\n",
    "    axs[1].set_xlabel(\"Sample Index\")\n",
    "    axs[1].set_ylabel(\"Gamma\")\n",
    "    axs[1].set_title(\"GAMMA: True vs. Predicted\")\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # 1) Train the model\n",
    "    model, train_losses, test_losses = train_model_with_eps(\n",
    "        N_train=3000,\n",
    "        N_test=300,\n",
    "        q_pts=28,          # fewer points => faster\n",
    "        range_q=(-4,4),\n",
    "        range_p=(-4,4),\n",
    "        eta_range=(0.5, 1.0),\n",
    "        gamma_range=(0.0, 1.0),\n",
    "        db_range=(10.0, 15.0),\n",
    "        n_epochs=30,\n",
    "        batch_size=16,\n",
    "        lr=1e-3,\n",
    "        hidden_dim=64,\n",
    "        Nphi=5,\n",
    "        phi_clip=3.0\n",
    "    )\n",
    "\n",
    "    # 2) Plot training curves\n",
    "    plot_training_performance(train_losses, test_losses)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_wigner_tomography(\n",
    "    epsilon: float,\n",
    "    in_state=(0.0, 0.0),\n",
    "    eta=1.0,\n",
    "    gamma=0.0,\n",
    "    q_pts=50,\n",
    "    range_q=(-5.0,5.0),\n",
    "    range_p=(-5.0,5.0),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare a single-mode GKP(epsilon, in_state), apply pure-loss channel(eta),\n",
    "    then approximate the dephasing channel by integrating over many rotation angles\n",
    "    phi ~ Normal(0, gamma). Returns a 2D Wigner array of shape (q_pts, q_pts).\n",
    "    \"\"\"\n",
    "    qvals = np.linspace(range_q[0], range_q[1], q_pts)\n",
    "    pvals = np.linspace(range_p[0], range_p[1], q_pts)\n",
    "\n",
    "    # If gamma is extremely small, skip the rotation sampling\n",
    "    if gamma < 1e-12:\n",
    "        prog = sf.Program(1)\n",
    "        with prog.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "        eng = sf.Engine(backend)\n",
    "        state = eng.run(prog).state\n",
    "        return state.wigner(0, qvals, pvals)\n",
    "\n",
    "    # Otherwise, discretize phi in ± phi_clip * sqrt(gamma)\n",
    "    phi_std = np.sqrt(gamma)\n",
    "    phi_min = -phi_clip * phi_std\n",
    "    phi_max = +phi_clip * phi_std\n",
    "    phis = np.linspace(phi_min, phi_max, Nphi)\n",
    "    dphi = phis[1] - phis[0]  # step size\n",
    "\n",
    "    # Gaussian weights\n",
    "    w_phi = np.exp(-0.5 * (phis/phi_std)**2)\n",
    "    w_phi /= (phi_std * np.sqrt(2*np.pi))  # normalization\n",
    "\n",
    "    W_accum = np.zeros((q_pts, q_pts), dtype=float)\n",
    "    for i, phi_val in enumerate(phis):\n",
    "        prog_phi = sf.Program(1)\n",
    "        with prog_phi.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "            sf.ops.Rgate(phi_val) | q[0]\n",
    "\n",
    "        eng_phi = sf.Engine(backend)\n",
    "        state_phi = eng_phi.run(prog_phi).state\n",
    "        W_phi = state_phi.wigner(0, qvals, pvals)\n",
    "\n",
    "        W_accum += w_phi[i]*W_phi\n",
    "\n",
    "    # Multiply by dphi to approximate integral\n",
    "    return W_accum * dphi\n",
    "\n",
    "def resource_cost(eps: float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the 'resource cost' for generating a GKP with damping param eps,\n",
    "    using cost = -10 * log10[ tanh(eps) ].\n",
    "    Larger eps => tanh(eps)->1 => cost->0, which is 'cheaper' to create.\n",
    "    Smaller eps => tanh(eps)->0 => cost-> +infinity => more resource needed.\n",
    "    \"\"\"\n",
    "    t = math.tanh(eps)\n",
    "    if t < 1e-12:\n",
    "        # Very large cost if eps is extremely small => extremely squeezed\n",
    "        return 50.0  # or some large number\n",
    "    return -10.0 * math.log10(t)\n",
    "\n",
    "\n",
    "def wigner_overlap(W_A: np.ndarray, \n",
    "                   W_B: np.ndarray, \n",
    "                   q_vals: np.ndarray, \n",
    "                   p_vals: np.ndarray) -> float:\n",
    "    dq = q_vals[1] - q_vals[0]\n",
    "    dp = p_vals[1] - p_vals[0]\n",
    "    \n",
    "    overlap = 2 * np.pi * np.sum(W_A * W_B) * dq * dp\n",
    "    \n",
    "    return max(0.0, min(1.0, overlap)) \n",
    "\n",
    "\n",
    "def compute_fidelity_ij(\n",
    "    eps: float, \n",
    "    i_state: str,  # \"0\" or \"1\"\n",
    "    j_state: str,  # \"0\" or \"1\"\n",
    "    eta: float,\n",
    "    gamma: float,\n",
    "    q_pts=28,\n",
    "    range_q=(-4,4),\n",
    "    range_p=(-4,4),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    F_{i,j}(eps) = < i^eps | rho_out(j^eps) | i^eps >,\n",
    "    computed by Wigner overlap:\n",
    "      W_in  = Wigner( i^eps, no channel )\n",
    "      W_out = Wigner( j^eps ) after channel(eta, gamma).\n",
    "\n",
    "    \"\"\"\n",
    "    # Choose the \"in_state\" positions\n",
    "    if i_state == \"0\":\n",
    "        in_st_i = (0, 0)\n",
    "    else:  # \"1\"\n",
    "        in_st_i = (np.pi, 0)\n",
    "\n",
    "    if j_state == \"0\":\n",
    "        in_st_j = (0, 0)\n",
    "    else:  # \"1\"\n",
    "        in_st_j = (np.pi, 0)\n",
    "\n",
    "    # (A) Wigner of i^eps (no channel)\n",
    "    W_i = compute_wigner_tomography(\n",
    "        epsilon = eps,\n",
    "        in_state = in_st_i,\n",
    "        eta = 1.0,\n",
    "        gamma = 0.0,\n",
    "        q_pts = q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=backend,\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "    # (B) Wigner of j^eps after channel\n",
    "    W_j_out = compute_wigner_tomography(\n",
    "        epsilon = eps,\n",
    "        in_state = in_st_j,\n",
    "        eta = eta,\n",
    "        gamma = gamma,\n",
    "        q_pts = q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=backend,\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "\n",
    "    # Overlap\n",
    "    q_vals = np.linspace(range_q[0], range_q[1], q_pts)\n",
    "    p_vals = np.linspace(range_p[0], range_p[1], q_pts)\n",
    "    dq = q_vals[1] - q_vals[0]\n",
    "    dp = p_vals[1] - p_vals[0]\n",
    "\n",
    "    W_i = W_i / (np.sum(W_i) * dq * dp)\n",
    "    W_j_out = W_j_out / (np.sum(W_j_out) * dq * dp)\n",
    "\n",
    "    # norm_Wi = np.sum(W_i) * dq * dp\n",
    "    # norm_Wj = np.sum(W_j_out) * dq * dp\n",
    "    # print(f\"Normalization check - W1: {norm_Wi:.6f}, W2: {norm_Wj:.6f}\")\n",
    "\n",
    "    fidelity = wigner_overlap(W_i, W_j_out, q_vals, p_vals)\n",
    "    \n",
    "    return fidelity\n",
    "\n",
    "\n",
    "def code_fidelity_objective(\n",
    "    eps: float,\n",
    "    eta_est: float,\n",
    "    gamma_est: float,\n",
    "    alpha=1.0,\n",
    "    beta=0.1,          \n",
    "    q_pts=28,\n",
    "    range_q=(-4,4),\n",
    "    range_p=(-4,4),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    O(eps) = \n",
    "       [0.5*F_{0,0}(eps) + 0.5*F_{1,1}(eps) - alpha*(F_{0,1}+F_{1,0}) ]\n",
    "       - beta * resource_cost(eps)\n",
    "\n",
    "    We maximize this with a ternary search. \n",
    "    Larger 'eps' => smaller resource cost => less penalty.\n",
    "    \"\"\"\n",
    "    # code fidelity part\n",
    "    F00 = compute_fidelity_ij(eps, \"0\",\"0\", eta_est, gamma_est,\n",
    "                              q_pts=q_pts, range_q=range_q, range_p=range_p,\n",
    "                              backend=backend, Nphi=Nphi, phi_clip=phi_clip)\n",
    "    F11 = compute_fidelity_ij(eps, \"1\",\"1\", eta_est, gamma_est,\n",
    "                              q_pts=q_pts, range_q=range_q, range_p=range_p,\n",
    "                              backend=backend, Nphi=Nphi, phi_clip=phi_clip)\n",
    "    F01 = compute_fidelity_ij(eps, \"0\",\"1\", eta_est, gamma_est,\n",
    "                              q_pts=q_pts, range_q=range_q, range_p=range_p,\n",
    "                              backend=backend, Nphi=Nphi, phi_clip=phi_clip)\n",
    "    F10 = compute_fidelity_ij(eps, \"1\",\"0\", eta_est, gamma_est,\n",
    "                              q_pts=q_pts, range_q=range_q, range_p=range_p,\n",
    "                              backend=backend, Nphi=Nphi, phi_clip=phi_clip)\n",
    "\n",
    "    fidelity_term = 0.5*(F00 + F11) - alpha*(F01 + F10)\n",
    "\n",
    "    # resource penalty\n",
    "    rcost = resource_cost(eps)  # from the function we defined\n",
    "    # final objective\n",
    "    obj_val = fidelity_term - beta*rcost\n",
    "\n",
    "    return obj_val\n",
    "\n",
    "\n",
    "\n",
    "def ternary_search_code_fidelity(\n",
    "    eta_est: float,\n",
    "    gamma_est: float,\n",
    "    alpha=1.0,\n",
    "    beta=0.1,          \n",
    "    eps_min=0.0,\n",
    "    eps_max=0.3,\n",
    "    tolerance=1e-3,\n",
    "    q_pts=28,\n",
    "    range_q=(-4,4),\n",
    "    range_p=(-4,4),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a ternary search on [eps_min, eps_max] to maximize\n",
    "    code_fidelity_objective(...).\n",
    "    We'll pass 'beta' to incorporate resource penalty.\n",
    "    \"\"\"\n",
    "    left = eps_min\n",
    "    right = eps_max\n",
    "\n",
    "    def code_obj(eps):\n",
    "        return code_fidelity_objective(\n",
    "            eps, eta_est, gamma_est, alpha=alpha, beta=beta,\n",
    "            q_pts=q_pts, range_q=range_q, range_p=range_p,\n",
    "            backend=backend, Nphi=Nphi, phi_clip=phi_clip\n",
    "        )\n",
    "\n",
    "    while (right - left) > tolerance:\n",
    "        m1 = left + (right - left)/3\n",
    "        m2 = right - (right - left)/3\n",
    "        f1 = code_obj(m1)\n",
    "        f2 = code_obj(m2)\n",
    "\n",
    "        if f1 < f2:\n",
    "            left = m1\n",
    "        else:\n",
    "            right = m2\n",
    "\n",
    "    eps_opt = 0.5*(left + right)\n",
    "    O_opt = code_obj(eps_opt)\n",
    "    return eps_opt, O_opt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.12282255e-06 -1.09487148e-07 -5.89090410e-05 -5.20986845e-03\n",
      "  -7.94856931e-02 -2.09203326e-01 -9.49873161e-02 -7.44012204e-03\n",
      "  -1.00533756e-04 -2.29750325e-07  5.19107157e-06  1.01114320e-03\n",
      "   3.39764377e-02  1.96952017e-01  1.96952017e-01  3.39764377e-02\n",
      "   1.01114320e-03  5.19107157e-06 -2.29750325e-07 -1.00533756e-04\n",
      "  -7.44012204e-03 -9.49873161e-02 -2.09203326e-01 -7.94856931e-02\n",
      "  -5.20986845e-03 -5.89090410e-05 -1.09487148e-07  5.12282255e-06]\n",
      " [ 9.08477652e-06 -1.94185491e-07 -1.04480030e-04 -9.24013025e-03\n",
      "  -1.40974415e-01 -3.71039308e-01 -1.68467819e-01 -1.31956685e-02\n",
      "  -1.78304887e-04 -4.07481972e-07  9.20580881e-06  1.79315405e-03\n",
      "   6.02535694e-02  3.49273284e-01  3.49273284e-01  6.02535694e-02\n",
      "   1.79315405e-03  9.20580881e-06 -4.07481972e-07 -1.78304887e-04\n",
      "  -1.31956685e-02 -1.68467819e-01 -3.71039308e-01 -1.40974415e-01\n",
      "  -9.24013025e-03 -1.04480030e-04 -1.94185491e-07  9.08477652e-06]\n",
      " [ 2.78264331e-06 -5.93355037e-08 -3.19286478e-05 -2.82374406e-03\n",
      "  -4.30811749e-02 -1.13388016e-01 -5.14830407e-02 -4.03253952e-03\n",
      "  -5.44892064e-05 -1.24519017e-07  2.81971526e-06  5.49238392e-04\n",
      "   1.84555106e-02  1.06981493e-01  1.06981493e-01  1.84555106e-02\n",
      "   5.49238392e-04  2.81971526e-06 -1.24519017e-07 -5.44892064e-05\n",
      "  -4.03253952e-03 -5.14830407e-02 -1.13388016e-01 -4.30811749e-02\n",
      "  -2.82374406e-03 -3.19286478e-05 -5.93355037e-08  2.78264331e-06]\n",
      " [ 4.28761829e-07  3.49042386e-09  1.55675000e-06  1.37677715e-04\n",
      "   2.10051534e-03  5.52847661e-03  2.51016638e-03  1.96615137e-04\n",
      "   2.65673853e-06  6.57775882e-09  4.34480089e-07  8.46282100e-05\n",
      "   2.84367744e-03  1.64840120e-02  1.64840120e-02  2.84367744e-03\n",
      "   8.46282100e-05  4.34480089e-07  6.57775882e-09  2.65673853e-06\n",
      "   1.96615137e-04  2.51016638e-03  5.52847661e-03  2.10051534e-03\n",
      "   1.37677715e-04  1.55675000e-06  3.49042386e-09  4.28761829e-07]\n",
      " [ 4.30472380e-06  1.01063628e-07  4.94753436e-05  4.37555965e-03\n",
      "   6.67568471e-02  1.75701487e-01  7.97760388e-02  6.24866023e-03\n",
      "   8.44342729e-05  2.00682807e-07  4.36216616e-06  8.49653969e-04\n",
      "   2.85500759e-02  1.65496897e-01  1.65496897e-01  2.85500759e-02\n",
      "   8.49653969e-04  4.36216616e-06  2.00682807e-07  8.44342729e-05\n",
      "   6.24866023e-03  7.97760388e-02  1.75701487e-01  6.67568471e-02\n",
      "   4.37555965e-03  4.94753436e-05  1.01063628e-07  4.30472380e-06]\n",
      " [ 1.13263576e-05  2.66070701e-07  1.30257616e-04  1.15198789e-02\n",
      "   1.75755985e-01  4.62583080e-01  2.10032632e-01  1.64513377e-02\n",
      "   2.22296730e-04  5.28347053e-07  1.14774969e-05  2.23556378e-03\n",
      "   7.51194229e-02  4.35446527e-01  4.35446527e-01  7.51194229e-02\n",
      "   2.23556378e-03  1.14774969e-05  5.28347053e-07  2.22296730e-04\n",
      "   1.64513377e-02  2.10032632e-01  4.62583080e-01  1.75755985e-01\n",
      "   1.15198789e-02  1.30257616e-04  2.66070701e-07  1.13263576e-05]\n",
      " [ 5.14368160e-06  1.20785586e-07  5.91307463e-05  5.22947571e-03\n",
      "   7.97848363e-02  2.09990660e-01  9.53447994e-02  7.46812283e-03\n",
      "   1.00912115e-04  2.39846212e-07  5.21231905e-06  1.01524504e-03\n",
      "   3.41142676e-02  1.97750978e-01  1.97750978e-01  3.41142676e-02\n",
      "   1.01524504e-03  5.21231905e-06  2.39846212e-07  1.00912115e-04\n",
      "   7.46812283e-03  9.53447994e-02  2.09990660e-01  7.97848363e-02\n",
      "   5.22947571e-03  5.91307463e-05  1.20785586e-07  5.14368160e-06]\n",
      " [ 5.37897245e-07  6.57511682e-09  3.07892576e-06  2.72297717e-04\n",
      "   4.15437989e-03  1.09341701e-02  4.96458395e-03  3.88863608e-04\n",
      "   5.25447296e-06  1.27311206e-08  5.45072062e-07  1.06169009e-04\n",
      "   3.56749145e-03  2.06797617e-02  2.06797617e-02  3.56749145e-03\n",
      "   1.06169009e-04  5.45072062e-07  1.27311206e-08  5.25447296e-06\n",
      "   3.88863608e-04  4.96458395e-03  1.09341701e-02  4.15437989e-03\n",
      "   2.72297717e-04  3.07892576e-06  6.57511682e-09  5.37897245e-07]\n",
      " [ 3.06405195e-06 -6.52494631e-08 -3.51131752e-05 -3.10538111e-03\n",
      "  -4.73780428e-02 -1.24697210e-01 -5.66179013e-02 -4.43474047e-03\n",
      "  -5.99238986e-05 -1.36934929e-07  3.10487302e-06  6.04782849e-04\n",
      "   2.03219156e-02  1.17800527e-01  1.17800527e-01  2.03219156e-02\n",
      "   6.04782849e-04  3.10487302e-06 -1.36934929e-07 -5.99238986e-05\n",
      "  -4.43474047e-03 -5.66179013e-02 -1.24697210e-01 -4.73780428e-02\n",
      "  -3.10538111e-03 -3.51131752e-05 -6.52494631e-08  3.06405195e-06]\n",
      " [ 1.19469085e-05 -2.55362860e-07 -1.37396054e-04 -1.21511971e-02\n",
      "  -1.85387853e-01 -4.87933789e-01 -2.21542945e-01 -1.73529121e-02\n",
      "  -2.34479143e-04 -5.35857564e-07  1.21060717e-05  2.35808194e-03\n",
      "   7.92362784e-02  4.59310801e-01  4.59310801e-01  7.92362784e-02\n",
      "   2.35808194e-03  1.21060717e-05 -5.35857564e-07 -2.34479143e-04\n",
      "  -1.73529121e-02 -2.21542945e-01 -4.87933789e-01 -1.85387853e-01\n",
      "  -1.21511971e-02 -1.37396054e-04 -2.55362860e-07  1.19469085e-05]\n",
      " [ 8.05042381e-06 -1.72064035e-07 -9.25780622e-05 -8.18752974e-03\n",
      "  -1.24915146e-01 -3.28771921e-01 -1.49276605e-01 -1.16924681e-02\n",
      "  -1.57993072e-04 -3.61062681e-07  8.15767591e-06  1.58899342e-03\n",
      "   5.33933629e-02  3.09506564e-01  3.09506564e-01  5.33933629e-02\n",
      "   1.58899342e-03  8.15767591e-06 -3.61062681e-07 -1.57993072e-04\n",
      "  -1.16924681e-02 -1.49276605e-01 -3.28771921e-01 -1.24915146e-01\n",
      "  -8.18752974e-03 -9.25780622e-05 -1.72064035e-07  8.05042381e-06]\n",
      " [ 9.90508628e-07 -1.87161047e-08 -1.01323989e-05 -8.96101249e-04\n",
      "  -1.36715984e-02 -3.59831278e-02 -1.63378890e-02 -1.27970653e-03\n",
      "  -1.72918809e-05 -3.94190617e-08  1.00370590e-06  1.95506524e-04\n",
      "   6.56941097e-03  3.80810592e-02  3.80810592e-02  6.56941097e-03\n",
      "   1.95506524e-04  1.00370590e-06 -3.94190617e-08 -1.72918809e-05\n",
      "  -1.27970653e-03 -1.63378890e-02 -3.59831278e-02 -1.36715984e-02\n",
      "  -8.96101249e-04 -1.01323989e-05 -1.87161047e-08  9.90508628e-07]\n",
      " [ 1.85806434e-06  4.28064238e-08  2.09368741e-05  1.85164032e-03\n",
      "   2.82500251e-02  7.43529935e-02  3.37594599e-02  2.64429517e-03\n",
      "   3.57307218e-05  8.49572052e-08  1.88285801e-06  3.66739435e-04\n",
      "   1.23231799e-02  7.14340670e-02  7.14340670e-02  1.23231799e-02\n",
      "   3.66739435e-04  1.88285801e-06  8.49572052e-08  3.57307218e-05\n",
      "   2.64429517e-03  3.37594599e-02  7.43529935e-02  2.82500251e-02\n",
      "   1.85164032e-03  2.09368741e-05  4.28064238e-08  1.85806434e-06]\n",
      " [ 1.06619737e-05  2.50460700e-07  1.22615517e-04  1.08440179e-02\n",
      "   1.65444538e-01  4.35443742e-01  1.97710204e-01  1.54861523e-02\n",
      "   2.09254778e-04  4.97349511e-07  1.08042474e-05  2.10442960e-03\n",
      "   7.07130516e-02  4.09904011e-01  4.09904011e-01  7.07130516e-02\n",
      "   2.10442960e-03  1.08042474e-05  4.97349511e-07  2.09254778e-04\n",
      "   1.54861523e-02  1.97710204e-01  4.35443742e-01  1.65444538e-01\n",
      "   1.08440179e-02  1.22615517e-04  2.50460700e-07  1.06619737e-05]\n",
      " [ 1.06619737e-05  2.50460700e-07  1.22615517e-04  1.08440179e-02\n",
      "   1.65444538e-01  4.35443742e-01  1.97710204e-01  1.54861523e-02\n",
      "   2.09254778e-04  4.97349511e-07  1.08042474e-05  2.10442960e-03\n",
      "   7.07130516e-02  4.09904011e-01  4.09904011e-01  7.07130516e-02\n",
      "   2.10442960e-03  1.08042474e-05  4.97349511e-07  2.09254778e-04\n",
      "   1.54861523e-02  1.97710204e-01  4.35443742e-01  1.65444538e-01\n",
      "   1.08440179e-02  1.22615517e-04  2.50460700e-07  1.06619737e-05]\n",
      " [ 1.85806434e-06  4.28064238e-08  2.09368741e-05  1.85164032e-03\n",
      "   2.82500251e-02  7.43529935e-02  3.37594599e-02  2.64429517e-03\n",
      "   3.57307218e-05  8.49572052e-08  1.88285801e-06  3.66739435e-04\n",
      "   1.23231799e-02  7.14340670e-02  7.14340670e-02  1.23231799e-02\n",
      "   3.66739435e-04  1.88285801e-06  8.49572052e-08  3.57307218e-05\n",
      "   2.64429517e-03  3.37594599e-02  7.43529935e-02  2.82500251e-02\n",
      "   1.85164032e-03  2.09368741e-05  4.28064238e-08  1.85806434e-06]\n",
      " [ 9.90508628e-07 -1.87161047e-08 -1.01323989e-05 -8.96101249e-04\n",
      "  -1.36715984e-02 -3.59831278e-02 -1.63378890e-02 -1.27970653e-03\n",
      "  -1.72918809e-05 -3.94190617e-08  1.00370590e-06  1.95506524e-04\n",
      "   6.56941097e-03  3.80810592e-02  3.80810592e-02  6.56941097e-03\n",
      "   1.95506524e-04  1.00370590e-06 -3.94190617e-08 -1.72918809e-05\n",
      "  -1.27970653e-03 -1.63378890e-02 -3.59831278e-02 -1.36715984e-02\n",
      "  -8.96101249e-04 -1.01323989e-05 -1.87161047e-08  9.90508628e-07]\n",
      " [ 8.05042381e-06 -1.72064035e-07 -9.25780622e-05 -8.18752974e-03\n",
      "  -1.24915146e-01 -3.28771921e-01 -1.49276605e-01 -1.16924681e-02\n",
      "  -1.57993072e-04 -3.61062681e-07  8.15767591e-06  1.58899342e-03\n",
      "   5.33933629e-02  3.09506564e-01  3.09506564e-01  5.33933629e-02\n",
      "   1.58899342e-03  8.15767591e-06 -3.61062681e-07 -1.57993072e-04\n",
      "  -1.16924681e-02 -1.49276605e-01 -3.28771921e-01 -1.24915146e-01\n",
      "  -8.18752974e-03 -9.25780622e-05 -1.72064035e-07  8.05042381e-06]\n",
      " [ 1.19469085e-05 -2.55362860e-07 -1.37396054e-04 -1.21511971e-02\n",
      "  -1.85387853e-01 -4.87933789e-01 -2.21542945e-01 -1.73529121e-02\n",
      "  -2.34479143e-04 -5.35857564e-07  1.21060717e-05  2.35808194e-03\n",
      "   7.92362784e-02  4.59310801e-01  4.59310801e-01  7.92362784e-02\n",
      "   2.35808194e-03  1.21060717e-05 -5.35857564e-07 -2.34479143e-04\n",
      "  -1.73529121e-02 -2.21542945e-01 -4.87933789e-01 -1.85387853e-01\n",
      "  -1.21511971e-02 -1.37396054e-04 -2.55362860e-07  1.19469085e-05]\n",
      " [ 3.06405195e-06 -6.52494631e-08 -3.51131752e-05 -3.10538111e-03\n",
      "  -4.73780428e-02 -1.24697210e-01 -5.66179013e-02 -4.43474047e-03\n",
      "  -5.99238986e-05 -1.36934929e-07  3.10487302e-06  6.04782849e-04\n",
      "   2.03219156e-02  1.17800527e-01  1.17800527e-01  2.03219156e-02\n",
      "   6.04782849e-04  3.10487302e-06 -1.36934929e-07 -5.99238986e-05\n",
      "  -4.43474047e-03 -5.66179013e-02 -1.24697210e-01 -4.73780428e-02\n",
      "  -3.10538111e-03 -3.51131752e-05 -6.52494631e-08  3.06405195e-06]\n",
      " [ 5.37897245e-07  6.57511682e-09  3.07892576e-06  2.72297717e-04\n",
      "   4.15437989e-03  1.09341701e-02  4.96458395e-03  3.88863608e-04\n",
      "   5.25447296e-06  1.27311206e-08  5.45072062e-07  1.06169009e-04\n",
      "   3.56749145e-03  2.06797617e-02  2.06797617e-02  3.56749145e-03\n",
      "   1.06169009e-04  5.45072062e-07  1.27311206e-08  5.25447296e-06\n",
      "   3.88863608e-04  4.96458395e-03  1.09341701e-02  4.15437989e-03\n",
      "   2.72297717e-04  3.07892576e-06  6.57511682e-09  5.37897245e-07]\n",
      " [ 5.14368160e-06  1.20785586e-07  5.91307463e-05  5.22947571e-03\n",
      "   7.97848363e-02  2.09990660e-01  9.53447994e-02  7.46812283e-03\n",
      "   1.00912115e-04  2.39846212e-07  5.21231905e-06  1.01524504e-03\n",
      "   3.41142676e-02  1.97750978e-01  1.97750978e-01  3.41142676e-02\n",
      "   1.01524504e-03  5.21231905e-06  2.39846212e-07  1.00912115e-04\n",
      "   7.46812283e-03  9.53447994e-02  2.09990660e-01  7.97848363e-02\n",
      "   5.22947571e-03  5.91307463e-05  1.20785586e-07  5.14368160e-06]\n",
      " [ 1.13263576e-05  2.66070701e-07  1.30257616e-04  1.15198789e-02\n",
      "   1.75755985e-01  4.62583080e-01  2.10032632e-01  1.64513377e-02\n",
      "   2.22296730e-04  5.28347053e-07  1.14774969e-05  2.23556378e-03\n",
      "   7.51194229e-02  4.35446527e-01  4.35446527e-01  7.51194229e-02\n",
      "   2.23556378e-03  1.14774969e-05  5.28347053e-07  2.22296730e-04\n",
      "   1.64513377e-02  2.10032632e-01  4.62583080e-01  1.75755985e-01\n",
      "   1.15198789e-02  1.30257616e-04  2.66070701e-07  1.13263576e-05]\n",
      " [ 4.30472380e-06  1.01063628e-07  4.94753436e-05  4.37555965e-03\n",
      "   6.67568471e-02  1.75701487e-01  7.97760388e-02  6.24866023e-03\n",
      "   8.44342729e-05  2.00682807e-07  4.36216616e-06  8.49653969e-04\n",
      "   2.85500759e-02  1.65496897e-01  1.65496897e-01  2.85500759e-02\n",
      "   8.49653969e-04  4.36216616e-06  2.00682807e-07  8.44342729e-05\n",
      "   6.24866023e-03  7.97760388e-02  1.75701487e-01  6.67568471e-02\n",
      "   4.37555965e-03  4.94753436e-05  1.01063628e-07  4.30472380e-06]\n",
      " [ 4.28761829e-07  3.49042386e-09  1.55675000e-06  1.37677715e-04\n",
      "   2.10051534e-03  5.52847661e-03  2.51016638e-03  1.96615137e-04\n",
      "   2.65673853e-06  6.57775882e-09  4.34480089e-07  8.46282100e-05\n",
      "   2.84367744e-03  1.64840120e-02  1.64840120e-02  2.84367744e-03\n",
      "   8.46282100e-05  4.34480089e-07  6.57775882e-09  2.65673853e-06\n",
      "   1.96615137e-04  2.51016638e-03  5.52847661e-03  2.10051534e-03\n",
      "   1.37677715e-04  1.55675000e-06  3.49042386e-09  4.28761829e-07]\n",
      " [ 2.78264331e-06 -5.93355037e-08 -3.19286478e-05 -2.82374406e-03\n",
      "  -4.30811749e-02 -1.13388016e-01 -5.14830407e-02 -4.03253952e-03\n",
      "  -5.44892064e-05 -1.24519017e-07  2.81971526e-06  5.49238392e-04\n",
      "   1.84555106e-02  1.06981493e-01  1.06981493e-01  1.84555106e-02\n",
      "   5.49238392e-04  2.81971526e-06 -1.24519017e-07 -5.44892064e-05\n",
      "  -4.03253952e-03 -5.14830407e-02 -1.13388016e-01 -4.30811749e-02\n",
      "  -2.82374406e-03 -3.19286478e-05 -5.93355037e-08  2.78264331e-06]\n",
      " [ 9.08477652e-06 -1.94185491e-07 -1.04480030e-04 -9.24013025e-03\n",
      "  -1.40974415e-01 -3.71039308e-01 -1.68467819e-01 -1.31956685e-02\n",
      "  -1.78304887e-04 -4.07481972e-07  9.20580881e-06  1.79315405e-03\n",
      "   6.02535694e-02  3.49273284e-01  3.49273284e-01  6.02535694e-02\n",
      "   1.79315405e-03  9.20580881e-06 -4.07481972e-07 -1.78304887e-04\n",
      "  -1.31956685e-02 -1.68467819e-01 -3.71039308e-01 -1.40974415e-01\n",
      "  -9.24013025e-03 -1.04480030e-04 -1.94185491e-07  9.08477652e-06]\n",
      " [ 5.12282255e-06 -1.09487148e-07 -5.89090410e-05 -5.20986845e-03\n",
      "  -7.94856931e-02 -2.09203326e-01 -9.49873161e-02 -7.44012204e-03\n",
      "  -1.00533756e-04 -2.29750325e-07  5.19107157e-06  1.01114320e-03\n",
      "   3.39764377e-02  1.96952017e-01  1.96952017e-01  3.39764377e-02\n",
      "   1.01114320e-03  5.19107157e-06 -2.29750325e-07 -1.00533756e-04\n",
      "  -7.44012204e-03 -9.49873161e-02 -2.09203326e-01 -7.94856931e-02\n",
      "  -5.20986845e-03 -5.89090410e-05 -1.09487148e-07  5.12282255e-06]]\n",
      "[[ 5.12282255e-06 -1.09487148e-07 -5.89090410e-05 -5.20986845e-03\n",
      "  -7.94856931e-02 -2.09203326e-01 -9.49873161e-02 -7.44012204e-03\n",
      "  -1.00533756e-04 -2.29750325e-07  5.19107157e-06  1.01114320e-03\n",
      "   3.39764377e-02  1.96952017e-01  1.96952017e-01  3.39764377e-02\n",
      "   1.01114320e-03  5.19107157e-06 -2.29750325e-07 -1.00533756e-04\n",
      "  -7.44012204e-03 -9.49873161e-02 -2.09203326e-01 -7.94856931e-02\n",
      "  -5.20986845e-03 -5.89090410e-05 -1.09487148e-07  5.12282255e-06]\n",
      " [ 9.08477652e-06 -1.94185491e-07 -1.04480030e-04 -9.24013025e-03\n",
      "  -1.40974415e-01 -3.71039308e-01 -1.68467819e-01 -1.31956685e-02\n",
      "  -1.78304887e-04 -4.07481972e-07  9.20580881e-06  1.79315405e-03\n",
      "   6.02535694e-02  3.49273284e-01  3.49273284e-01  6.02535694e-02\n",
      "   1.79315405e-03  9.20580881e-06 -4.07481972e-07 -1.78304887e-04\n",
      "  -1.31956685e-02 -1.68467819e-01 -3.71039308e-01 -1.40974415e-01\n",
      "  -9.24013025e-03 -1.04480030e-04 -1.94185491e-07  9.08477652e-06]\n",
      " [ 2.78264331e-06 -5.93355037e-08 -3.19286478e-05 -2.82374406e-03\n",
      "  -4.30811749e-02 -1.13388016e-01 -5.14830407e-02 -4.03253952e-03\n",
      "  -5.44892064e-05 -1.24519017e-07  2.81971526e-06  5.49238392e-04\n",
      "   1.84555106e-02  1.06981493e-01  1.06981493e-01  1.84555106e-02\n",
      "   5.49238392e-04  2.81971526e-06 -1.24519017e-07 -5.44892064e-05\n",
      "  -4.03253952e-03 -5.14830407e-02 -1.13388016e-01 -4.30811749e-02\n",
      "  -2.82374406e-03 -3.19286478e-05 -5.93355037e-08  2.78264331e-06]\n",
      " [ 4.28761829e-07  3.49042386e-09  1.55675000e-06  1.37677715e-04\n",
      "   2.10051534e-03  5.52847661e-03  2.51016638e-03  1.96615137e-04\n",
      "   2.65673853e-06  6.57775882e-09  4.34480089e-07  8.46282100e-05\n",
      "   2.84367744e-03  1.64840120e-02  1.64840120e-02  2.84367744e-03\n",
      "   8.46282100e-05  4.34480089e-07  6.57775882e-09  2.65673853e-06\n",
      "   1.96615137e-04  2.51016638e-03  5.52847661e-03  2.10051534e-03\n",
      "   1.37677715e-04  1.55675000e-06  3.49042386e-09  4.28761829e-07]\n",
      " [ 4.30472380e-06  1.01063628e-07  4.94753436e-05  4.37555965e-03\n",
      "   6.67568471e-02  1.75701487e-01  7.97760388e-02  6.24866023e-03\n",
      "   8.44342729e-05  2.00682807e-07  4.36216616e-06  8.49653969e-04\n",
      "   2.85500759e-02  1.65496897e-01  1.65496897e-01  2.85500759e-02\n",
      "   8.49653969e-04  4.36216616e-06  2.00682807e-07  8.44342729e-05\n",
      "   6.24866023e-03  7.97760388e-02  1.75701487e-01  6.67568471e-02\n",
      "   4.37555965e-03  4.94753436e-05  1.01063628e-07  4.30472380e-06]\n",
      " [ 1.13263576e-05  2.66070701e-07  1.30257616e-04  1.15198789e-02\n",
      "   1.75755985e-01  4.62583080e-01  2.10032632e-01  1.64513377e-02\n",
      "   2.22296730e-04  5.28347053e-07  1.14774969e-05  2.23556378e-03\n",
      "   7.51194229e-02  4.35446527e-01  4.35446527e-01  7.51194229e-02\n",
      "   2.23556378e-03  1.14774969e-05  5.28347053e-07  2.22296730e-04\n",
      "   1.64513377e-02  2.10032632e-01  4.62583080e-01  1.75755985e-01\n",
      "   1.15198789e-02  1.30257616e-04  2.66070701e-07  1.13263576e-05]\n",
      " [ 5.14368160e-06  1.20785586e-07  5.91307463e-05  5.22947571e-03\n",
      "   7.97848363e-02  2.09990660e-01  9.53447994e-02  7.46812283e-03\n",
      "   1.00912115e-04  2.39846212e-07  5.21231905e-06  1.01524504e-03\n",
      "   3.41142676e-02  1.97750978e-01  1.97750978e-01  3.41142676e-02\n",
      "   1.01524504e-03  5.21231905e-06  2.39846212e-07  1.00912115e-04\n",
      "   7.46812283e-03  9.53447994e-02  2.09990660e-01  7.97848363e-02\n",
      "   5.22947571e-03  5.91307463e-05  1.20785586e-07  5.14368160e-06]\n",
      " [ 5.37897245e-07  6.57511682e-09  3.07892576e-06  2.72297717e-04\n",
      "   4.15437989e-03  1.09341701e-02  4.96458395e-03  3.88863608e-04\n",
      "   5.25447296e-06  1.27311206e-08  5.45072062e-07  1.06169009e-04\n",
      "   3.56749145e-03  2.06797617e-02  2.06797617e-02  3.56749145e-03\n",
      "   1.06169009e-04  5.45072062e-07  1.27311206e-08  5.25447296e-06\n",
      "   3.88863608e-04  4.96458395e-03  1.09341701e-02  4.15437989e-03\n",
      "   2.72297717e-04  3.07892576e-06  6.57511682e-09  5.37897245e-07]\n",
      " [ 3.06405195e-06 -6.52494631e-08 -3.51131752e-05 -3.10538111e-03\n",
      "  -4.73780428e-02 -1.24697210e-01 -5.66179013e-02 -4.43474047e-03\n",
      "  -5.99238986e-05 -1.36934929e-07  3.10487302e-06  6.04782849e-04\n",
      "   2.03219156e-02  1.17800527e-01  1.17800527e-01  2.03219156e-02\n",
      "   6.04782849e-04  3.10487302e-06 -1.36934929e-07 -5.99238986e-05\n",
      "  -4.43474047e-03 -5.66179013e-02 -1.24697210e-01 -4.73780428e-02\n",
      "  -3.10538111e-03 -3.51131752e-05 -6.52494631e-08  3.06405195e-06]\n",
      " [ 1.19469085e-05 -2.55362860e-07 -1.37396054e-04 -1.21511971e-02\n",
      "  -1.85387853e-01 -4.87933789e-01 -2.21542945e-01 -1.73529121e-02\n",
      "  -2.34479143e-04 -5.35857564e-07  1.21060717e-05  2.35808194e-03\n",
      "   7.92362784e-02  4.59310801e-01  4.59310801e-01  7.92362784e-02\n",
      "   2.35808194e-03  1.21060717e-05 -5.35857564e-07 -2.34479143e-04\n",
      "  -1.73529121e-02 -2.21542945e-01 -4.87933789e-01 -1.85387853e-01\n",
      "  -1.21511971e-02 -1.37396054e-04 -2.55362860e-07  1.19469085e-05]\n",
      " [ 8.05042381e-06 -1.72064035e-07 -9.25780622e-05 -8.18752974e-03\n",
      "  -1.24915146e-01 -3.28771921e-01 -1.49276605e-01 -1.16924681e-02\n",
      "  -1.57993072e-04 -3.61062681e-07  8.15767591e-06  1.58899342e-03\n",
      "   5.33933629e-02  3.09506564e-01  3.09506564e-01  5.33933629e-02\n",
      "   1.58899342e-03  8.15767591e-06 -3.61062681e-07 -1.57993072e-04\n",
      "  -1.16924681e-02 -1.49276605e-01 -3.28771921e-01 -1.24915146e-01\n",
      "  -8.18752974e-03 -9.25780622e-05 -1.72064035e-07  8.05042381e-06]\n",
      " [ 9.90508628e-07 -1.87161047e-08 -1.01323989e-05 -8.96101249e-04\n",
      "  -1.36715984e-02 -3.59831278e-02 -1.63378890e-02 -1.27970653e-03\n",
      "  -1.72918809e-05 -3.94190617e-08  1.00370590e-06  1.95506524e-04\n",
      "   6.56941097e-03  3.80810592e-02  3.80810592e-02  6.56941097e-03\n",
      "   1.95506524e-04  1.00370590e-06 -3.94190617e-08 -1.72918809e-05\n",
      "  -1.27970653e-03 -1.63378890e-02 -3.59831278e-02 -1.36715984e-02\n",
      "  -8.96101249e-04 -1.01323989e-05 -1.87161047e-08  9.90508628e-07]\n",
      " [ 1.85806434e-06  4.28064238e-08  2.09368741e-05  1.85164032e-03\n",
      "   2.82500251e-02  7.43529935e-02  3.37594599e-02  2.64429517e-03\n",
      "   3.57307218e-05  8.49572052e-08  1.88285801e-06  3.66739435e-04\n",
      "   1.23231799e-02  7.14340670e-02  7.14340670e-02  1.23231799e-02\n",
      "   3.66739435e-04  1.88285801e-06  8.49572052e-08  3.57307218e-05\n",
      "   2.64429517e-03  3.37594599e-02  7.43529935e-02  2.82500251e-02\n",
      "   1.85164032e-03  2.09368741e-05  4.28064238e-08  1.85806434e-06]\n",
      " [ 1.06619737e-05  2.50460700e-07  1.22615517e-04  1.08440179e-02\n",
      "   1.65444538e-01  4.35443742e-01  1.97710204e-01  1.54861523e-02\n",
      "   2.09254778e-04  4.97349511e-07  1.08042474e-05  2.10442960e-03\n",
      "   7.07130516e-02  4.09904011e-01  4.09904011e-01  7.07130516e-02\n",
      "   2.10442960e-03  1.08042474e-05  4.97349511e-07  2.09254778e-04\n",
      "   1.54861523e-02  1.97710204e-01  4.35443742e-01  1.65444538e-01\n",
      "   1.08440179e-02  1.22615517e-04  2.50460700e-07  1.06619737e-05]\n",
      " [ 1.06619737e-05  2.50460700e-07  1.22615517e-04  1.08440179e-02\n",
      "   1.65444538e-01  4.35443742e-01  1.97710204e-01  1.54861523e-02\n",
      "   2.09254778e-04  4.97349511e-07  1.08042474e-05  2.10442960e-03\n",
      "   7.07130516e-02  4.09904011e-01  4.09904011e-01  7.07130516e-02\n",
      "   2.10442960e-03  1.08042474e-05  4.97349511e-07  2.09254778e-04\n",
      "   1.54861523e-02  1.97710204e-01  4.35443742e-01  1.65444538e-01\n",
      "   1.08440179e-02  1.22615517e-04  2.50460700e-07  1.06619737e-05]\n",
      " [ 1.85806434e-06  4.28064238e-08  2.09368741e-05  1.85164032e-03\n",
      "   2.82500251e-02  7.43529935e-02  3.37594599e-02  2.64429517e-03\n",
      "   3.57307218e-05  8.49572052e-08  1.88285801e-06  3.66739435e-04\n",
      "   1.23231799e-02  7.14340670e-02  7.14340670e-02  1.23231799e-02\n",
      "   3.66739435e-04  1.88285801e-06  8.49572052e-08  3.57307218e-05\n",
      "   2.64429517e-03  3.37594599e-02  7.43529935e-02  2.82500251e-02\n",
      "   1.85164032e-03  2.09368741e-05  4.28064238e-08  1.85806434e-06]\n",
      " [ 9.90508628e-07 -1.87161047e-08 -1.01323989e-05 -8.96101249e-04\n",
      "  -1.36715984e-02 -3.59831278e-02 -1.63378890e-02 -1.27970653e-03\n",
      "  -1.72918809e-05 -3.94190617e-08  1.00370590e-06  1.95506524e-04\n",
      "   6.56941097e-03  3.80810592e-02  3.80810592e-02  6.56941097e-03\n",
      "   1.95506524e-04  1.00370590e-06 -3.94190617e-08 -1.72918809e-05\n",
      "  -1.27970653e-03 -1.63378890e-02 -3.59831278e-02 -1.36715984e-02\n",
      "  -8.96101249e-04 -1.01323989e-05 -1.87161047e-08  9.90508628e-07]\n",
      " [ 8.05042381e-06 -1.72064035e-07 -9.25780622e-05 -8.18752974e-03\n",
      "  -1.24915146e-01 -3.28771921e-01 -1.49276605e-01 -1.16924681e-02\n",
      "  -1.57993072e-04 -3.61062681e-07  8.15767591e-06  1.58899342e-03\n",
      "   5.33933629e-02  3.09506564e-01  3.09506564e-01  5.33933629e-02\n",
      "   1.58899342e-03  8.15767591e-06 -3.61062681e-07 -1.57993072e-04\n",
      "  -1.16924681e-02 -1.49276605e-01 -3.28771921e-01 -1.24915146e-01\n",
      "  -8.18752974e-03 -9.25780622e-05 -1.72064035e-07  8.05042381e-06]\n",
      " [ 1.19469085e-05 -2.55362860e-07 -1.37396054e-04 -1.21511971e-02\n",
      "  -1.85387853e-01 -4.87933789e-01 -2.21542945e-01 -1.73529121e-02\n",
      "  -2.34479143e-04 -5.35857564e-07  1.21060717e-05  2.35808194e-03\n",
      "   7.92362784e-02  4.59310801e-01  4.59310801e-01  7.92362784e-02\n",
      "   2.35808194e-03  1.21060717e-05 -5.35857564e-07 -2.34479143e-04\n",
      "  -1.73529121e-02 -2.21542945e-01 -4.87933789e-01 -1.85387853e-01\n",
      "  -1.21511971e-02 -1.37396054e-04 -2.55362860e-07  1.19469085e-05]\n",
      " [ 3.06405195e-06 -6.52494631e-08 -3.51131752e-05 -3.10538111e-03\n",
      "  -4.73780428e-02 -1.24697210e-01 -5.66179013e-02 -4.43474047e-03\n",
      "  -5.99238986e-05 -1.36934929e-07  3.10487302e-06  6.04782849e-04\n",
      "   2.03219156e-02  1.17800527e-01  1.17800527e-01  2.03219156e-02\n",
      "   6.04782849e-04  3.10487302e-06 -1.36934929e-07 -5.99238986e-05\n",
      "  -4.43474047e-03 -5.66179013e-02 -1.24697210e-01 -4.73780428e-02\n",
      "  -3.10538111e-03 -3.51131752e-05 -6.52494631e-08  3.06405195e-06]\n",
      " [ 5.37897245e-07  6.57511682e-09  3.07892576e-06  2.72297717e-04\n",
      "   4.15437989e-03  1.09341701e-02  4.96458395e-03  3.88863608e-04\n",
      "   5.25447296e-06  1.27311206e-08  5.45072062e-07  1.06169009e-04\n",
      "   3.56749145e-03  2.06797617e-02  2.06797617e-02  3.56749145e-03\n",
      "   1.06169009e-04  5.45072062e-07  1.27311206e-08  5.25447296e-06\n",
      "   3.88863608e-04  4.96458395e-03  1.09341701e-02  4.15437989e-03\n",
      "   2.72297717e-04  3.07892576e-06  6.57511682e-09  5.37897245e-07]\n",
      " [ 5.14368160e-06  1.20785586e-07  5.91307463e-05  5.22947571e-03\n",
      "   7.97848363e-02  2.09990660e-01  9.53447994e-02  7.46812283e-03\n",
      "   1.00912115e-04  2.39846212e-07  5.21231905e-06  1.01524504e-03\n",
      "   3.41142676e-02  1.97750978e-01  1.97750978e-01  3.41142676e-02\n",
      "   1.01524504e-03  5.21231905e-06  2.39846212e-07  1.00912115e-04\n",
      "   7.46812283e-03  9.53447994e-02  2.09990660e-01  7.97848363e-02\n",
      "   5.22947571e-03  5.91307463e-05  1.20785586e-07  5.14368160e-06]\n",
      " [ 1.13263576e-05  2.66070701e-07  1.30257616e-04  1.15198789e-02\n",
      "   1.75755985e-01  4.62583080e-01  2.10032632e-01  1.64513377e-02\n",
      "   2.22296730e-04  5.28347053e-07  1.14774969e-05  2.23556378e-03\n",
      "   7.51194229e-02  4.35446527e-01  4.35446527e-01  7.51194229e-02\n",
      "   2.23556378e-03  1.14774969e-05  5.28347053e-07  2.22296730e-04\n",
      "   1.64513377e-02  2.10032632e-01  4.62583080e-01  1.75755985e-01\n",
      "   1.15198789e-02  1.30257616e-04  2.66070701e-07  1.13263576e-05]\n",
      " [ 4.30472380e-06  1.01063628e-07  4.94753436e-05  4.37555965e-03\n",
      "   6.67568471e-02  1.75701487e-01  7.97760388e-02  6.24866023e-03\n",
      "   8.44342729e-05  2.00682807e-07  4.36216616e-06  8.49653969e-04\n",
      "   2.85500759e-02  1.65496897e-01  1.65496897e-01  2.85500759e-02\n",
      "   8.49653969e-04  4.36216616e-06  2.00682807e-07  8.44342729e-05\n",
      "   6.24866023e-03  7.97760388e-02  1.75701487e-01  6.67568471e-02\n",
      "   4.37555965e-03  4.94753436e-05  1.01063628e-07  4.30472380e-06]\n",
      " [ 4.28761829e-07  3.49042386e-09  1.55675000e-06  1.37677715e-04\n",
      "   2.10051534e-03  5.52847661e-03  2.51016638e-03  1.96615137e-04\n",
      "   2.65673853e-06  6.57775882e-09  4.34480089e-07  8.46282100e-05\n",
      "   2.84367744e-03  1.64840120e-02  1.64840120e-02  2.84367744e-03\n",
      "   8.46282100e-05  4.34480089e-07  6.57775882e-09  2.65673853e-06\n",
      "   1.96615137e-04  2.51016638e-03  5.52847661e-03  2.10051534e-03\n",
      "   1.37677715e-04  1.55675000e-06  3.49042386e-09  4.28761829e-07]\n",
      " [ 2.78264331e-06 -5.93355037e-08 -3.19286478e-05 -2.82374406e-03\n",
      "  -4.30811749e-02 -1.13388016e-01 -5.14830407e-02 -4.03253952e-03\n",
      "  -5.44892064e-05 -1.24519017e-07  2.81971526e-06  5.49238392e-04\n",
      "   1.84555106e-02  1.06981493e-01  1.06981493e-01  1.84555106e-02\n",
      "   5.49238392e-04  2.81971526e-06 -1.24519017e-07 -5.44892064e-05\n",
      "  -4.03253952e-03 -5.14830407e-02 -1.13388016e-01 -4.30811749e-02\n",
      "  -2.82374406e-03 -3.19286478e-05 -5.93355037e-08  2.78264331e-06]\n",
      " [ 9.08477652e-06 -1.94185491e-07 -1.04480030e-04 -9.24013025e-03\n",
      "  -1.40974415e-01 -3.71039308e-01 -1.68467819e-01 -1.31956685e-02\n",
      "  -1.78304887e-04 -4.07481972e-07  9.20580881e-06  1.79315405e-03\n",
      "   6.02535694e-02  3.49273284e-01  3.49273284e-01  6.02535694e-02\n",
      "   1.79315405e-03  9.20580881e-06 -4.07481972e-07 -1.78304887e-04\n",
      "  -1.31956685e-02 -1.68467819e-01 -3.71039308e-01 -1.40974415e-01\n",
      "  -9.24013025e-03 -1.04480030e-04 -1.94185491e-07  9.08477652e-06]\n",
      " [ 5.12282255e-06 -1.09487148e-07 -5.89090410e-05 -5.20986845e-03\n",
      "  -7.94856931e-02 -2.09203326e-01 -9.49873161e-02 -7.44012204e-03\n",
      "  -1.00533756e-04 -2.29750325e-07  5.19107157e-06  1.01114320e-03\n",
      "   3.39764377e-02  1.96952017e-01  1.96952017e-01  3.39764377e-02\n",
      "   1.01114320e-03  5.19107157e-06 -2.29750325e-07 -1.00533756e-04\n",
      "  -7.44012204e-03 -9.49873161e-02 -2.09203326e-01 -7.94856931e-02\n",
      "  -5.20986845e-03 -5.89090410e-05 -1.09487148e-07  5.12282255e-06]]\n",
      "Normalization check - W1: 1.000000, W2: 1.000000\n",
      "Fidelity is 1.0\n"
     ]
    }
   ],
   "source": [
    "def compute_wigner_tomography(\n",
    "    epsilon: float,\n",
    "    in_state=(0.0, 0.0),\n",
    "    eta=1.0,\n",
    "    gamma=0.0,\n",
    "    q_pts=50,\n",
    "    range_q=(-5.0,5.0),\n",
    "    range_p=(-5.0,5.0),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare a single-mode GKP(epsilon, in_state), apply pure-loss channel(eta),\n",
    "    then approximate the dephasing channel by integrating over many rotation angles\n",
    "    phi ~ Normal(0, gamma). Returns a 2D Wigner array of shape (q_pts, q_pts).\n",
    "    \"\"\"\n",
    "    qvals = np.linspace(range_q[0], range_q[1], q_pts)\n",
    "    pvals = np.linspace(range_p[0], range_p[1], q_pts)\n",
    "\n",
    "    # If gamma is extremely small, skip the rotation sampling\n",
    "    if gamma < 1e-12:\n",
    "        prog = sf.Program(1)\n",
    "        with prog.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "        eng = sf.Engine(backend)\n",
    "        state = eng.run(prog).state\n",
    "        return state.wigner(0, qvals, pvals)\n",
    "\n",
    "    # Otherwise, discretize phi in ± phi_clip * sqrt(gamma)\n",
    "    phi_std = np.sqrt(gamma)\n",
    "    phi_min = -phi_clip * phi_std\n",
    "    phi_max = +phi_clip * phi_std\n",
    "    phis = np.linspace(phi_min, phi_max, Nphi)\n",
    "    dphi = phis[1] - phis[0]  # step size\n",
    "\n",
    "    # Gaussian weights\n",
    "    w_phi = np.exp(-0.5 * (phis/phi_std)**2)\n",
    "    w_phi /= (phi_std * np.sqrt(2*np.pi))  # normalization\n",
    "\n",
    "    W_accum = np.zeros((q_pts, q_pts), dtype=float)\n",
    "    for i, phi_val in enumerate(phis):\n",
    "        prog_phi = sf.Program(1)\n",
    "        with prog_phi.context as q:\n",
    "            sf.ops.GKP(state=in_state, epsilon=epsilon) | q[0]\n",
    "            if eta < 1.0:\n",
    "                sf.ops.LossChannel(eta) | q[0]\n",
    "            sf.ops.Rgate(phi_val) | q[0]\n",
    "\n",
    "        eng_phi = sf.Engine(backend)\n",
    "        state_phi = eng_phi.run(prog_phi).state\n",
    "        W_phi = state_phi.wigner(0, qvals, pvals)\n",
    "\n",
    "        W_accum += w_phi[i]*W_phi\n",
    "\n",
    "    # Multiply by dphi to approximate integral\n",
    "    return W_accum * dphi\n",
    "def compute_fidelity_test(\n",
    "    eps: float, \n",
    "    i_state: str,  # \"0\" or \"1\"\n",
    "    q_pts=28,\n",
    "    range_q=(-4,4),\n",
    "    range_p=(-4,4),\n",
    "    backend=\"bosonic\",\n",
    "    Nphi=5,\n",
    "    phi_clip=3.0\n",
    ") -> float:\n",
    "    \n",
    "    # Choose the \"in_state\" positions\n",
    "    if i_state == \"0\":\n",
    "        in_st_i = (0, 0)\n",
    "    else:  # \"1\"\n",
    "        in_st_i = (np.pi, 0)\n",
    "\n",
    "    # (A) Wigner of i^eps (no channel)\n",
    "    W_1 = compute_wigner_tomography(\n",
    "        epsilon = eps,\n",
    "        in_state = in_st_i,\n",
    "        eta = 1.0,\n",
    "        gamma = 0.0,\n",
    "        q_pts = q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=backend,\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "\n",
    "    # (A) Wigner of i^eps (no channel)\n",
    "    W_2 = compute_wigner_tomography(\n",
    "        epsilon = eps,\n",
    "        in_state = in_st_i,\n",
    "        eta = 1.0,\n",
    "        gamma = 0.0,\n",
    "        q_pts = q_pts,\n",
    "        range_q=range_q,\n",
    "        range_p=range_p,\n",
    "        backend=backend,\n",
    "        Nphi=Nphi,\n",
    "        phi_clip=phi_clip\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Overlap\n",
    "    q_vals = np.linspace(range_q[0], range_q[1], q_pts)\n",
    "    p_vals = np.linspace(range_p[0], range_p[1], q_pts)\n",
    "    dq = q_vals[1] - q_vals[0]\n",
    "    dp = p_vals[1] - p_vals[0]\n",
    "\n",
    "    W_1 = W_1 / (np.sum(W_1) * dq * dp)\n",
    "    W_2 = W_2 / (np.sum(W_2) * dq * dp)\n",
    "    print(W_1)\n",
    "    print(W_2)\n",
    "\n",
    "    norm_W1 = np.sum(W_1) * dq * dp\n",
    "    norm_W2 = np.sum(W_2) * dq * dp\n",
    "    print(f\"Normalization check - W1: {norm_W1:.6f}, W2: {norm_W2:.6f}\")\n",
    "\n",
    "    fidelity = wigner_overlap(W_1, W_2, q_vals, p_vals)\n",
    "    \n",
    "    return fidelity\n",
    "\n",
    "\n",
    "test_fidel = compute_fidelity_test(0.05, \"0\")\n",
    "print(f\"Fidelity is {test_fidel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING THE MODEL...\n",
      "Epoch 1/30: train MSE=0.099590, test MSE=0.032241\n",
      "Epoch 2/30: train MSE=0.014868, test MSE=0.006064\n",
      "Epoch 3/30: train MSE=0.004469, test MSE=0.002765\n",
      "Epoch 4/30: train MSE=0.002642, test MSE=0.001824\n",
      "Epoch 5/30: train MSE=0.001757, test MSE=0.001132\n",
      "Epoch 6/30: train MSE=0.001067, test MSE=0.000657\n",
      "Epoch 7/30: train MSE=0.000661, test MSE=0.000424\n",
      "Epoch 8/30: train MSE=0.000457, test MSE=0.000331\n",
      "Epoch 9/30: train MSE=0.000350, test MSE=0.000256\n",
      "Epoch 10/30: train MSE=0.000283, test MSE=0.000291\n",
      "Epoch 11/30: train MSE=0.000236, test MSE=0.000183\n",
      "Epoch 12/30: train MSE=0.000205, test MSE=0.000189\n",
      "Epoch 13/30: train MSE=0.000175, test MSE=0.000149\n",
      "Epoch 14/30: train MSE=0.000157, test MSE=0.000139\n",
      "Epoch 15/30: train MSE=0.000139, test MSE=0.000120\n",
      "Epoch 16/30: train MSE=0.000131, test MSE=0.000096\n",
      "Epoch 17/30: train MSE=0.000114, test MSE=0.000106\n",
      "Epoch 18/30: train MSE=0.000131, test MSE=0.000090\n",
      "Epoch 19/30: train MSE=0.000110, test MSE=0.000131\n",
      "Epoch 20/30: train MSE=0.000096, test MSE=0.000083\n",
      "Epoch 21/30: train MSE=0.000095, test MSE=0.000104\n",
      "Epoch 22/30: train MSE=0.000090, test MSE=0.000076\n",
      "Epoch 23/30: train MSE=0.000093, test MSE=0.000069\n",
      "Epoch 24/30: train MSE=0.000073, test MSE=0.000061\n",
      "Epoch 25/30: train MSE=0.000070, test MSE=0.000074\n",
      "Epoch 26/30: train MSE=0.000067, test MSE=0.000072\n",
      "Epoch 27/30: train MSE=0.000076, test MSE=0.000068\n",
      "Epoch 28/30: train MSE=0.000069, test MSE=0.000085\n",
      "Epoch 29/30: train MSE=0.000064, test MSE=0.000061\n",
      "Epoch 30/30: train MSE=0.000064, test MSE=0.000111\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def full_protocol_demo():\n",
    "    \"\"\"\n",
    "    1) Train the NN to learn mapping from (Wigner, eps) => (eta, gamma).\n",
    "    2) Suppose we transmit a single state with some *unknown* channel (eta_true, gamma_true).\n",
    "    3) We measure the output's Wigner + eps, feed it to the NN => (eta_est, gamma_est).\n",
    "    4) Then we run ternary search over epsilon in [0, 0.3] to find eps_opt maximizing fidelity.\n",
    "    5) Print results.\n",
    "\n",
    "    This is a toy demonstration in code. In an actual experiment, steps 2-3\n",
    "    correspond to physically generating the state, measuring it, etc.\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # A) Train the model\n",
    "    # -------------------------\n",
    "    print(\"TRAINING THE MODEL...\")\n",
    "    model, train_losses, test_losses = train_model_with_eps(\n",
    "        N_train=3000,\n",
    "        N_test=300,\n",
    "        q_pts=28,          # fewer points => faster\n",
    "        range_q=(-4,4),\n",
    "        range_p=(-4,4),\n",
    "        eta_range=(0.5, 1.0),\n",
    "        gamma_range=(0.0, 1.0),\n",
    "        db_range=(10.0, 15.0),\n",
    "        n_epochs=30,\n",
    "        batch_size=16,\n",
    "        lr=1e-3,\n",
    "        hidden_dim=64,\n",
    "        Nphi=5,\n",
    "        phi_clip=3.0\n",
    "    )\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_training_performance(train_losses, test_losses)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = full_protocol_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Simulating a single test example with ===\n",
      "    (eta_true=0.8, gamma_true=0.4, db_true=10.0)\n",
      "NN predicted: (eta_est=0.792, gamma_est=0.389)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "    # B) Simulate \"unknown\" channel for a single example\n",
    "    #    e.g. (eta_true=0.8, gamma_true=0.4), db_input=12\n",
    "    #    We pretend we measure the resulting Wigner and pass it to the NN\n",
    "    # -------------------------\n",
    "eta_true   = 0.8\n",
    "gamma_true = 0.4\n",
    "db_true    = 10.0  # we used GKP with ~10 dB\n",
    "print(\"\\n=== Simulating a single test example with ===\")\n",
    "print(f\"    (eta_true={eta_true}, gamma_true={gamma_true}, db_true={db_true})\")\n",
    "\n",
    "# Generate Wigner data, feed to NN => estimated (eta_est, gamma_est)\n",
    "eta_est, gamma_est = predict_single_example_with_eps(\n",
    "    trained_model,\n",
    "    eta_true   = eta_true,\n",
    "    gamma_true = gamma_true,\n",
    "    db_val     = db_true,\n",
    "    q_pts      = 28,\n",
    "    range_q    = (-4,4),\n",
    "    range_p    = (-4,4),\n",
    "    Nphi       = 5,\n",
    "    phi_clip   = 3.0\n",
    ")\n",
    "\n",
    "print(f\"NN predicted: (eta_est={eta_est:.3f}, gamma_est={gamma_est:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CODE FIDELITY + Resource Penalty]\n",
      "  alpha=1.0, beta=2\n",
      "  => eps_opt=0.4996, O_opt=-6.6159\n",
      "  F00=0.4618, F11=0.2836, F01=0.1454, F10=0.1326\n",
      "  Resource(dB) at eps_opt=3.3553 dB (penalty)\n"
     ]
    }
   ],
   "source": [
    "alpha_val = 1.0\n",
    "beta_val  = 2\n",
    "\n",
    "eps_min = 0.0\n",
    "eps_max = 0.5\n",
    "\n",
    "eps_opt, O_opt = ternary_search_code_fidelity(\n",
    "    eta_est   = eta_est,\n",
    "    gamma_est = gamma_est,\n",
    "    alpha     = alpha_val,\n",
    "    beta      = beta_val,\n",
    "    eps_min   = eps_min,\n",
    "    eps_max   = eps_max,\n",
    "    tolerance = 1e-3,\n",
    "    q_pts     = 28,\n",
    "    range_q   = (-4,4),\n",
    "    range_p   = (-4,4),\n",
    "    backend   = \"bosonic\",\n",
    "    Nphi      = 5,\n",
    "    phi_clip  = 3.0\n",
    ")\n",
    "\n",
    "print(f\"\\n[CODE FIDELITY + Resource Penalty]\")\n",
    "print(f\"  alpha={alpha_val}, beta={beta_val}\")\n",
    "print(f\"  => eps_opt={eps_opt:.4f}, O_opt={O_opt:.4f}\")\n",
    "\n",
    "# Evaluate separate fidelity terms at eps_opt:\n",
    "F00_opt = compute_fidelity_ij(eps_opt, \"0\",\"0\", eta_est, gamma_est, q_pts=28)\n",
    "F11_opt = compute_fidelity_ij(eps_opt, \"1\",\"1\", eta_est, gamma_est, q_pts=28)\n",
    "F01_opt = compute_fidelity_ij(eps_opt, \"0\",\"1\", eta_est, gamma_est, q_pts=28)\n",
    "F10_opt = compute_fidelity_ij(eps_opt, \"1\",\"0\", eta_est, gamma_est, q_pts=28)\n",
    "\n",
    "rc = resource_cost(eps_opt)\n",
    "print(f\"  F00={F00_opt:.4f}, F11={F11_opt:.4f}, F01={F01_opt:.4f}, F10={F10_opt:.4f}\")\n",
    "print(f\"  Resource(dB) at eps_opt={rc:.4f} dB (penalty)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_Computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
